{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70981328-59b9-464d-9cf0-89962aa25f51",
   "metadata": {},
   "source": [
    "## House Keeping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bcefea-e000-42d2-a2e1-c3348deb6f03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44753557-75ca-4cb8-8c3f-f568e3b31496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Dec  8 01:16:52 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   40C    P0    47W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513fbdee-7909-477c-bd32-d67aadaa628b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fffccef-7597-49a2-afb1-70727f534bd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bitsandbytes in ./.local/lib/python3.8/site-packages (0.35.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in ./.local/lib/python3.8/site-packages (0.13.6)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in ./.local/lib/python3.8/site-packages (from wandb) (1.0.11)\n",
      "Requirement already satisfied: promise<3,>=2.0 in ./.local/lib/python3.8/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in ./.local/lib/python3.8/site-packages (from wandb) (1.11.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./.local/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in ./.local/lib/python3.8/site-packages (from wandb) (3.20.0)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in ./.local/lib/python3.8/site-packages (from wandb) (3.1.29)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/lib/python3/dist-packages (from wandb) (7.0)\n",
      "Requirement already satisfied: pathtools in ./.local/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/lib/python3/dist-packages (from wandb) (5.5.1)\n",
      "Requirement already satisfied: setproctitle in ./.local/lib/python3.8/site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./.local/lib/python3.8/site-packages (from wandb) (2.28.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (45.2.0)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from wandb) (5.3.1)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.14.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./.local/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./.local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.local/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.26.13)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./.local/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes\n",
    "!pip install wandb\n",
    "!pip install -q -r community-events/whisper-fine-tuning-event/requirements.txt\n",
    "!pip install --quiet datasets git+https://github.com/huggingface/transformers evaluate huggingface_hub jiwer bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53b5219-0869-462c-80e0-1297bd0a2ff0",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a6a7d10-7f4e-4032-9ada-a758e305a2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from datasets import Audio, interleave_datasets, IterableDataset, load_dataset, IterableDatasetDict\n",
    "from typing import List, Optional\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import WhisperProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "799903e5-db5f-49ab-ab45-7e0e09e2f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamDatasetLoader:\n",
    "    def __init__(self, dataset_names, dataset_config_names, dataset_splits, text_column_names, stopping_strategy=\"all_exhausted\"):\n",
    "        \"\"\"\n",
    "        Initialize DatasetLoader instance.\n",
    "\n",
    "        Args:\n",
    "            dataset_names (list): list of dataset names\n",
    "            dataset_config_names (list): list of dataset config names\n",
    "            dataset_splits (list): list of dataset splits\n",
    "            text_column_names (list): list of text column names\n",
    "            stopping_strategy (str, optional): stopping strategy for interleaved datasets. Defaults to \"all_exhausted\".\n",
    "        \"\"\"\n",
    "        self.dataset_names = dataset_names\n",
    "        self.dataset_config_names = dataset_config_names\n",
    "        self.splits = dataset_splits\n",
    "        self.text_column_names = text_column_names\n",
    "        self.stopping_strategy = stopping_strategy\n",
    "\n",
    "    def load_and_process_datasets(self):\n",
    "        \"\"\"\n",
    "        Load and process datasets.\n",
    "\n",
    "        Returns:\n",
    "            tuple: tuple of interleaved dataset and all datasets\n",
    "        \"\"\"\n",
    "        # Use list comprehension to create splits and text_column_names lists\n",
    "        splits = [\"train\" for i in range(len(self.dataset_names))] if self.splits is None else self.splits\n",
    "        text_column_names = [\"text\" for i in range(len(self.dataset_names))] if self.text_column_names is None else self.text_column_names\n",
    "\n",
    "        all_datasets = []\n",
    "        for i, dataset_name in tqdm(enumerate(self.dataset_names)):\n",
    "            for split in splits[i]:\n",
    "                dataset = load_dataset(path=self.dataset_names[i], name=self.dataset_config_names[i], split=split, streaming=True, use_auth_token=True)\n",
    "                # cast \"audio\" column to 'Audio' type with sampling rate of 16Mhz\n",
    "                dataset = dataset.cast_column(\"audio\", Audio(16000))\n",
    "                # rename text column to \"sentence\" if not already named so\n",
    "                if text_column_names[i] != \"sentence\":\n",
    "                    dataset = dataset.rename_column(text_column_names[i], \"sentence\")\n",
    "                # Store set of keys to be removed in a variable\n",
    "                keys_to_remove = set(dataset.features.keys()) - set([\"audio\", \"sentence\"])\n",
    "                # Use stored variable to remove columns\n",
    "                dataset = dataset.remove_columns(keys_to_remove)\n",
    "                all_datasets.append(dataset)\n",
    "\n",
    "        interleaved_dataset = interleave_datasets(all_datasets, stopping_strategy=self.stopping_strategy)\n",
    "        return interleaved_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b708fe4-f9dd-4593-986c-14314bce21d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9d74d7cf5041098f5499530fa82af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a6428f5d9f4e649ea5f88dde02bec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a534957aa6b41eaa97b0b7a05f66722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/12.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad19e315454e47858ba6276d52ff1640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.44k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3aec819e3c4cc1ae50aaca74ad967a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/60.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647ebf273ae047cf95ccca09942ff72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/12.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0328a2dc54ff412c86e076124c2d28aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_names = [\"mozilla-foundation/common_voice_11_0\", \"google/fleurs\"]\n",
    "dataset_config_names = [\"sw\", \"sw_ke\"]\n",
    "dataset_splits = [[\"train\", \"validation\"], [\"train\", \"validation\", \"test\"]]\n",
    "text_column_names = [\"sentence\", \"transcription\"]\n",
    "\n",
    "dataset_loader = StreamDatasetLoader(dataset_names, dataset_config_names, dataset_splits, text_column_names)\n",
    "stream_dataset = dataset_loader.load_and_process_datasets()\n",
    "\n",
    "# for i, sample in enumerate(stream_dataset):\n",
    "#     print(i, sample[\"sentence\"])\n",
    "#     if i == 9:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "744fa3d6-1cec-45da-8208-1865125e86da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c6576c17f848719d4c737226dabf9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca86d82233f34a3a82269e87fc3ffa4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da28de4c1cf44c8c9f997b1101dc0a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2c5e798de14b69a7d58cf783472a16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb5e7cc83734bb091e7c7fe480c4d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8254525a556e486892356b146762aab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b56aa29a104a50a787fd79b5c4551b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import WhisperProcessor\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "class NormalizeTranscriptions(object):\n",
    "    def __init__(self, do_lower_case, do_remove_punctuation):\n",
    "        self.processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\", language=\"sw\", task=\"transcribe\")\n",
    "        self.normalizer = BasicTextNormalizer()\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.do_remove_punctuation = do_remove_punctuation\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        # optional pre-processing steps\n",
    "        self.transcription = batch[\"sentence\"]\n",
    "        if self.do_lower_case:\n",
    "            self.transcription = self.transcription.lower()\n",
    "        if self.do_remove_punctuation:\n",
    "            self.transcription = self.normalizer(self.transcription).strip()\n",
    "        batch[\"sentence\"] = self.transcription\n",
    "        batch['labels'] = self.processor.tokenizer(self.transcription).input_ids\n",
    "\n",
    "        return batch\n",
    "\n",
    "normalizer = NormalizeTranscriptions(do_lower_case=True, do_remove_punctuation=True)\n",
    "normalizing_function = lambda x: normalizer(x)\n",
    "\n",
    "ds = stream_dataset.map(normalizing_function)\n",
    "\n",
    "# for i, sample in enumerate(ds):\n",
    "#   print(i, sample['sentence'])\n",
    "#   if i == 5:\n",
    "#     break\n",
    "\n",
    "# for i, sample in enumerate(ds):\n",
    "#   print(i, sample['labels'])\n",
    "#   if i == 5:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad8b823a-9613-4fac-a5fe-828efd02af51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    # load and (possibly) resample audio datato 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "    # compute input length of audio sample in seconds\n",
    "    batch[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "        \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "518f1bf0-f675-4c83-ba58-60069ac098b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f012d6c052b546868c07b22fb7890884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/185k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1248ebaa1c274034801b3eaa8b0602a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/829 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6912acc6d10b48acac1abb70e694f664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31590f1882dd4480879edefdcfa94820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0dafa6b6cf490eb1940a0e6a5e5f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2d83516151458482beb8ff83bc7749",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550095b370a14cf09faad5e2f8ce8f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Swahili\", task=\"transcribe\")\n",
    "vectorized_datasets = ds.map(prepare_dataset, remove_columns=['audio', 'sentence']).with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad5d5204-0cc4-4884-92f1-37bb3055d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_datasets = vectorized_datasets.shuffle(\n",
    "    buffer_size=500,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19251c47-a72a-4c41-89b7-bbbf3201b08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 30.0\n",
    "\n",
    "def is_audio_in_length_range(length):\n",
    "    return length < max_input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc504dfd-4749-403a-b8c2-95a4e086c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_datasets = vectorized_datasets.filter(\n",
    "    is_audio_in_length_range,\n",
    "    input_columns=[\"input_length\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e44775b-6178-4932-9380-c635a47fd0c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3648a6b12374e488c6df5a3e76d385a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 10238it [00:00, 31549.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['labels', 'input_features', 'input_length'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# streaming\n",
    "dataset_names = [\"mozilla-foundation/common_voice_11_0\"]\n",
    "dataset_config_names = [\"sw\"]\n",
    "dataset_splits = [[\"test\"]]\n",
    "text_column_names = [\"sentence\"]\n",
    "\n",
    "dataset_loader = StreamDatasetLoader(dataset_names, dataset_config_names, dataset_splits, text_column_names)\n",
    "stream_dataset = dataset_loader.load_and_process_datasets()\n",
    "\n",
    "# normalization\n",
    "stream_dataset = stream_dataset.map(normalizing_function)\n",
    "\n",
    "# pre-processing\n",
    "vectorized_test_datasets = stream_dataset.map(prepare_dataset, remove_columns=['audio', 'sentence']).with_format(\"torch\")\n",
    "\n",
    "next(iter(vectorized_test_datasets)).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ceb0567-5317-4cf2-827d-17561bbc084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a6764b9-c264-47c5-9f7a-4866550611d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e13ea4bd-5e04-4a0b-9738-8c82d8e9e0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8818d2aabb77481094aab00781615391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4bd0750-83e7-446e-9cf1-8cb8612977ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate with the 'normalised' WER\n",
    "do_normalize_eval = True\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True, normalize=do_normalize_eval)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True, normalize=do_normalize_eval)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae417ef2-e7ba-4e0b-bce1-c90591a56049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282d0c62efd442a5887fd8271ed2e26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.97k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ecc4bb32cf144a8afbeff4b7878aaec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pretrained checkpoint\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b15985fb-5aa3-40aa-b718-528adb6f30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97d42594-dc31-4649-828d-554c93a16282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.mkdir('./whisper-small-sw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74dc9218-4209-4ca3-80b6-b7cecd4e9b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./whisper-small-sw\",  # your repo name\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=400,\n",
    "    max_steps=3000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"wandb\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6128cd6d-ed11-4ed0-8c93-a754958ef737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "from transformers.trainer_pt_utils import IterableDatasetShard\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "# trainer callback to reinitialise and reshuffle the streamable datasets at the beginning of each epoch\n",
    "class ShuffleCallback(TrainerCallback):\n",
    "    def on_epoch_begin(self, args, state, control, train_dataloader, **kwargs):\n",
    "        if isinstance(train_dataloader.dataset, IterableDatasetShard):\n",
    "            pass  # set_epoch() is handled by the Trainer\n",
    "        elif isinstance(train_dataloader.dataset, IterableDataset):\n",
    "            train_dataloader.dataset.set_epoch(train_dataloader.dataset._epoch + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9fb2ba0-b695-43d6-bc7a-460a9485c5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=vectorized_datasets,\n",
    "    eval_dataset=vectorized_test_datasets,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    "    callbacks=[ShuffleCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21a19249-c84b-4448-a42e-a41749df1dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./whisper-small-sw/config.json\n",
      "Model weights saved in ./whisper-small-sw/pytorch_model.bin\n",
      "Feature extractor saved in ./whisper-small-sw/preprocessor_config.json\n",
      "tokenizer config file saved in ./whisper-small-sw/tokenizer_config.json\n",
      "Special tokens file saved in ./whisper-small-sw/special_tokens_map.json\n",
      "added tokens file saved in ./whisper-small-sw/added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(training_args.output_dir)\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a6e91c0-2d13-495b-9029-ac07448061ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecab4270-a51d-40c4-90ea-5bed0f6bd378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:27enh24w) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">cerulean-breeze-12</strong>: <a href=\"https://wandb.ai/mldude/whisper-small-sw/runs/27enh24w\" target=\"_blank\">https://wandb.ai/mldude/whisper-small-sw/runs/27enh24w</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221208_012240-27enh24w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:27enh24w). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ff1e2f61bd438299e55b325536e42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668430100003205, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/wandb/run-20221208_012257-3bpih5oc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mldude/whisper-small-sw/runs/3bpih5oc\" target=\"_blank\">clear-armadillo-13</a></strong> to <a href=\"https://wandb.ai/mldude/whisper-small-sw\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/mldude/whisper-small-sw/runs/3bpih5oc?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f49f279d8e0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W&B argument tracking\n",
    "config = dict(\n",
    "    output_dir=\"./whisper-small-sw\",  # your repo name\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=2,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=400,\n",
    "    max_steps=3000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"wandb\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# start a new W&B training run\n",
    "wandb.init(\n",
    "    project=\"whisper-small-sw\", \n",
    "    entity=\"mldude\", \n",
    "    tags=[\"whisper-event\", \"whisper-small-es\", \"Swahili\"],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4850293a-7f03-4bd0-bbda-d2f9c85c1187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/mldude/whisper-small-sw/runs/3bpih5oc?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x7f4a4c729be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 192000\n",
      "  Num Epochs = 9223372036854775807\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 3000\n",
      "  Number of trainable parameters = 241734912\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "Reading metadata...: 26614it [00:00, 47934.99it/s]\n",
      "Reading metadata...: 10233it [00:00, 29387.17it/s]\n",
      "The following columns in the training set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: input_length. If input_length are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 13:36:44, Epoch 1/9223372036854775807]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.412005</td>\n",
       "      <td>26.043352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.039100</td>\n",
       "      <td>0.366616</td>\n",
       "      <td>22.314021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.016700</td>\n",
       "      <td>0.355822</td>\n",
       "      <td>21.934203</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading metadata...: 10233it [00:01, 5234.85it/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 8\n",
      "Reading metadata...: 10238it [00:00, 29139.64it/s]\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: input_length. If input_length are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./whisper-small-sw/checkpoint-1000\n",
      "Configuration saved in ./whisper-small-sw/checkpoint-1000/config.json\n",
      "Model weights saved in ./whisper-small-sw/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in ./whisper-small-sw/checkpoint-1000/preprocessor_config.json\n",
      "tokenizer config file saved in ./whisper-small-sw/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./whisper-small-sw/checkpoint-1000/special_tokens_map.json\n",
      "added tokens file saved in ./whisper-small-sw/checkpoint-1000/added_tokens.json\n",
      "Reading metadata...: 10233it [00:00, 38235.03it/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples: Unknown\n",
      "  Batch size = 8\n",
      "Reading metadata...: 10238it [00:00, 27617.18it/s]\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: input_length. If input_length are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "Saving model checkpoint to ./whisper-small-sw/checkpoint-3000\n",
      "Configuration saved in ./whisper-small-sw/checkpoint-3000/config.json\n",
      "Model weights saved in ./whisper-small-sw/checkpoint-3000/pytorch_model.bin\n",
      "Feature extractor saved in ./whisper-small-sw/checkpoint-3000/preprocessor_config.json\n",
      "tokenizer config file saved in ./whisper-small-sw/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./whisper-small-sw/checkpoint-3000/special_tokens_map.json\n",
      "added tokens file saved in ./whisper-small-sw/checkpoint-3000/added_tokens.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./whisper-small-sw/checkpoint-3000 (score: 21.93420312804801).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=0.18269595941901207, metrics={'train_runtime': 49063.7464, 'train_samples_per_second': 3.913, 'train_steps_per_second': 0.061, 'total_flos': 5.540579959283712e+19, 'train_loss': 0.18269595941901207, 'epoch': 1.31})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%wandb\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6aa59e8-af78-4afb-926d-f9cbb8603841",
   "metadata": {},
   "outputs": [],
   "source": [
    "## push weights into HuggingFace repository\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "CN.MODEL_NAME = \"whisper-large-sw\" # Whisper variant\n",
    "CN.MODEL_PATH = Path(\"C:/Users/Hedronstone/Desktop/whisper_event\") / \"pretrained_weights\" / \"whisper-small-sw\" / \"checkpoint-3000\" # path to weights folder\n",
    "CN.REPO_TYPE = \"model\" # repository type, `model` or `space`\n",
    "CN.REPO_ID = \"hedronstone/\" + CN.MODEL_NAME # repository id\n",
    "\n",
    "# load saved weights into model\n",
    "model = model.from_pretrained(model_path)\n",
    "\n",
    "# prepare upload pipeline\n",
    "api = HfApi()\n",
    "\n",
    "# launch upload task\n",
    "api.upload_folder(\n",
    "    folder_path=model_path,\n",
    "    path_in_repo=\"weights/pytorch\",\n",
    "    repo_id=\"hedronstone/whisper-small-sw\",\n",
    "    repo_type=\"model\",\n",
    "    create_pr=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d6548-2f92-407f-835e-bb475ab07691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
