{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd91600-b01f-4caa-86a9-88abec4b436b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utils / Housekeeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35cc9a0-6dea-41af-9111-32d829e8e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import torchaudio\n",
    "import torchaudio.transforms as at\n",
    "import re\n",
    "\n",
    "import concurrent.futures\n",
    "import glob\n",
    "\n",
    "import os\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# from tqdm.notebook import tqdm   # for colab\n",
    "from tqdm import tqdm              # for jupyter\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from utils import CfgNode\n",
    "\n",
    "from typing import List, Union, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6c0416-51a8-47c8-819f-fc048bcc88f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03deff5f-c91b-49c8-b4bb-2b6bfd3be879",
   "metadata": {},
   "source": [
    "Since we are ustilizing HuggingFace's AudioLoader for staging our audio files and transcriptions, we'll need to pair each audio file's path with its corresponding transcript and save this information in a .txt file. Let's code up some functions for handling this pre-processing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6840ef3f-9ee5-4dc5-81d9-29e50ac9e857",
   "metadata": {},
   "source": [
    "We'll be using three locally stored datasets for this training task. This notebook is to contain a training pipeline for data stored _locally_. If you'd like to train a Whisper model using Lambda Cloud SLI, see the notebook `lambda_cloud_whisper_train.ipynb`. \n",
    "\n",
    "The three datasets are KenSpeech, Broadcast News Swahili, and Babel Swahili Language Pack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2e7d44-f779-4429-867b-bfb2b2bed6ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91365aa2-5bc2-4f66-9b31-5e5c5f8ccf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bracketed_text(string: str, return_removed_text: bool=False) -> Union[str, Tuple[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Remove text in angle brackets from a string and return the cleaned string.\n",
    "    Optionally, return the list of removed text.\n",
    "    \n",
    "    :param string: The input string.\n",
    "    :param return_removed_text: If True, return the list of removed text.\n",
    "    :return: The cleaned string and, optionally, the list of removed text.\n",
    "    \"\"\"\n",
    "    # use the re.findall() method to find all matches of text in angle brackets\n",
    "    bracketed_text = re.findall(r\"<.*?>\", string)\n",
    "    \n",
    "    # use the re.sub() method to remove text in angle brackets from the string\n",
    "    cleaned_string = re.sub(r\"<.*?>\", \"\", string)\n",
    "    \n",
    "    # return the cleaned string and the list of removed text\n",
    "    return (cleaned_string, list(set(bracketed_text))) if return_removed_text==True else (cleaned_string)\n",
    "\n",
    "\n",
    "def remove_extra_whitespace(lines: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Removes extra whitespace from a list of lines.    \n",
    "    Args:\n",
    "        lines: A list of strings containing the lines to process.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings with extra whitespace removed.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the processed lines\n",
    "    processed_lines = []\n",
    "\n",
    "    # Iterate over the lines\n",
    "    for line in lines:\n",
    "        # Replace multiple whitespace characters with a single space\n",
    "        line = re.sub(r\"\\s+\", \" \", line)\n",
    "    \n",
    "        # Strip leading and trailing whitespace from the line\n",
    "        line = line.strip()\n",
    "\n",
    "        # Add the processed line to the list\n",
    "        processed_lines.append(line)\n",
    "  \n",
    "    # Return the list of processed lines\n",
    "    return processed_lines\n",
    "\n",
    "\n",
    "def remove_parentheses(lines: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    This function removes the \"(())\" pattern from the given lines.\n",
    "    Args:\n",
    "        lines (List[str]): A list of strings containing the \"(())\" pattern.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings with the \"(())\" pattern removed.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the processed lines\n",
    "    processed_lines = []\n",
    "\n",
    "    # Iterate over the lines\n",
    "    for line in lines:\n",
    "        # Replace the \"(())\" pattern with an empty string\n",
    "        line = line.replace(\"(())\", \"\")\n",
    "\n",
    "        # Add the processed line to the list\n",
    "        processed_lines.append(line)\n",
    "  \n",
    "    # Return the list of processed lines\n",
    "    return processed_lines\n",
    "\n",
    "\n",
    "def find_wav_files(root_folder: str, ending=\".wav\") -> List[str]:\n",
    "    \"\"\"\n",
    "    Searches the folder tree starting at the specified root folder, and returns\n",
    "    a list of paths to all of the .wav files that it finds.\n",
    "    \"\"\"\n",
    "    # Create an empty list to store the paths\n",
    "    wav_file_paths = []\n",
    "\n",
    "    # Iterate over the folders and files in the tree\n",
    "    for path, folders, files in os.walk(root_folder):\n",
    "      # Iterate over the files in the current folder\n",
    "      for file in files:\n",
    "        # Check if the file has the .wav extension\n",
    "        if file.endswith(ending):\n",
    "          # Add the path to the .wav file to the list\n",
    "          wav_file_paths.append(os.path.join(path, file))\n",
    "\n",
    "    return wav_file_paths\n",
    "\n",
    "\n",
    "def extract_filenames(file_paths: List[str], name_to_extract: str=\".wav\") -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    This function takes in a list of file paths and an optional string of the file extension to extract. It returns a tuple of two lists - \n",
    "    the first list contains the extracted filenames, and the second list contains the paths of files that couldn't be found.\n",
    "    Args:\n",
    "    file_paths: a list of file paths\n",
    "    name_to_extract: a string of the file extension to extract (default is \".wav\")\n",
    "\n",
    "    Returns:\n",
    "    A tuple of two lists - the first list contains the extracted filenames, and the second list contains the paths of files that couldn't be found.\n",
    "    \"\"\"\n",
    "    filenames = []\n",
    "    failed_paths = []\n",
    "    for file_path in file_paths:\n",
    "        try:\n",
    "            # Split the file path into a list of strings using the \"\\\\\" separator\n",
    "            split_path = file_path.split(\"\\\\\")\n",
    "            # Get the last element of the split path (i.e. the filename)\n",
    "            filename = split_path[-1]\n",
    "            # Remove the \".wav\" extension from the filename\n",
    "            filename = filename.replace(name_to_extract, \"\")\n",
    "            # Add the extracted filename to the list of filenames\n",
    "            filenames.append(filename)\n",
    "        except FileNotFoundError:\n",
    "            # If a FileNotFoundError is raised, add the file path to the list of failed paths\n",
    "            failed_paths.append(file_path)\n",
    "    if len(failed_paths) > 0:\n",
    "        print(f\"{len(failed_paths)} paths weren't found.\")\n",
    "    return filenames, failed_paths\n",
    "\n",
    "\n",
    "def process_lines(lines: List[str]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Given a list of lines in the format 'filename=utterance',\n",
    "    this function returns a list of tuples containing the audio file name and the utterance.\n",
    "    \"\"\"  \n",
    "    # Initialize an empty list to store the processed lines\n",
    "    processed_lines = []\n",
    "  \n",
    "    # Iterate over the lines\n",
    "    for line in lines:\n",
    "        \n",
    "        # Split the line at the first '=' character\n",
    "        parts = line.split('=', 1)\n",
    "    \n",
    "        # Get the audio file name and the utterance from the parts\n",
    "        audio_file = parts[0] + \".wav\"\n",
    "        utterance = parts[1].strip()  # Use str.strip() to remove leading and trailing whitespace\n",
    "    \n",
    "        # Add the audio file name and the utterance to the list of processed lines\n",
    "        processed_lines.append((audio_file, utterance))\n",
    "  \n",
    "    # Return the list of processed lines\n",
    "    return processed_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400f63d8-f7a5-4a96-a70f-e2afc0e69074",
   "metadata": {
    "tags": []
   },
   "source": [
    "### KenSpeech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e029875e-f7f2-4105-95c3-7e9b90f8ce27",
   "metadata": {},
   "source": [
    "This code defines a function named `find_wav_files()` that takes a root folder as an input, and returns a list of paths to all of the .wav files that it finds in the folder tree. The function uses the os.walk() function to iterate over the folders and files in the tree, and appends the path to any .wav files that it finds to the wav_file_paths list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea707f30-20f2-4417-a24f-994e979bb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where to start our '.wav' search\n",
    "root_folder = r\"C:\\Users\\Hedronstone\\Desktop\\whisper_event\\datasets\\swahili\\kenspeech\\audios\"\n",
    "\n",
    "kenspeech_wav_files = find_wav_files(root_folder)\n",
    "kenspeech_wav_files[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1636137-df6c-4cac-8eb5-cd547a67500f",
   "metadata": {},
   "source": [
    "There is but one folder containing all transcripts that we'll need to search. The files's naming convention follows this format: `sample_*.txt` or `tweet_*.txt` where `*` denotes each file's unique id. We'll use the paths collected from our `find_wav_files` function and extract the file names into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef652a0-19cc-40b4-a571-1348bc4e0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames, files_not_found = extract_filenames(kenspeech_wav_files)\n",
    "\n",
    "print(f\"Length of filenames: {len(filenames)}. \\nSamples:\")\n",
    "print(filenames[:10]) # Should print [\"tweet_*\", \"tweet_*\", ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73b5c4-fc45-4657-8309-77f89f24920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptions_path = r\"C:\\Users\\Hedronstone\\Desktop\\whisper_event\\datasets\\swahili\\kenspeech\\transcripts\"\n",
    "transcriptions_paths = [\n",
    "    transcriptions_path + \"\\\\\" + text_file_name  + \".txt\" for text_file_name in filenames\n",
    "]\n",
    "\n",
    "print(f\"No. of transcriptions: {len(transcriptions_paths)}\")\n",
    "print(transcriptions_paths[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c2cff-d42d-4695-b758-8951099b4572",
   "metadata": {},
   "source": [
    "In this updated version of the `TextProcessor` class, we added a new method called process_text that takes a list of transcript paths as an input and processes the text in each transcript file.\n",
    "\n",
    "The process_text method first reads the lines of text from each transcript file, and stores the lines in the text_set list. Then, it uses the remove_parentheses, clean_strings, and remove_extra_spaces methods to process each line of text, removing parentheses, extra whitespace, and extra spaces. Finally, it prints the processed text to the console.\n",
    "\n",
    "To use the process_text method, we create a `TextProcessor` object, and then call the process_text method on the object, passing a list of transcript paths as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67d8aa2-5101-4092-a6f0-a2590ff62889",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self):\n",
    "        import re\n",
    "        self.re = re\n",
    "\n",
    "    def remove_parentheses(self, string):\n",
    "        \"\"\"Remove all text in parentheses from the given string.\"\"\"\n",
    "        return self.re.sub(r\"\\([^)]*\\)\", \"\", string)\n",
    "\n",
    "    def clean_strings(self, string_list):\n",
    "        \"\"\" Remove \\n and leading/trailing whitespace from strings using list comprehension\"\"\"\n",
    "        clean_list = [string.replace(\"\\n\", \"\").strip() for string in string_list]\n",
    "\n",
    "        return clean_list\n",
    "\n",
    "    def remove_extra_spaces(self, string_list):\n",
    "        \"\"\"Replace two or more consecutive spaces with a single space in the strings in the given list.\"\"\"\n",
    "        # use regular expression to match two or more spaces\n",
    "        pattern = self.re.compile(r\"\\s{2,}\")\n",
    "\n",
    "        # use list comprehension to apply pattern.sub to each string in string_list\n",
    "        processed_list = [pattern.sub(\" \", string) for string in string_list]\n",
    "\n",
    "        return processed_list\n",
    "\n",
    "    def process_text(self, transcript_paths):\n",
    "        # create a list to store the paths that raised a FileNotFoundError\n",
    "        not_found = []\n",
    "\n",
    "        text_set = []\n",
    "        for i in range(len(transcript_paths)):\n",
    "            try:\n",
    "                # try to open the file and read the lines\n",
    "                with open(transcript_paths[i]) as f:\n",
    "                    text = f.readlines()\n",
    "                    text_set.append(text)\n",
    "            except FileNotFoundError:\n",
    "                # if a FileNotFoundError is raised, append the path to the not_found list\n",
    "                not_found.append(transcript_paths[i])\n",
    "\n",
    "        # remove `(tweet_*)` from samples\n",
    "        # use generator expression to process text\n",
    "        new_text_set = self.remove_extra_spaces(\n",
    "            self.clean_strings(\n",
    "                (self.remove_parentheses(text_set[idx][0]) for idx, text in enumerate(text_set))\n",
    "        ))\n",
    "\n",
    "        return new_text_set\n",
    "    \n",
    "def find_strings_with_substring(strings: List[str], substring: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    This function takes a list of strings and a substring and returns a list of strings from the original list\n",
    "    that contain the substring.\n",
    "    Args:\n",
    "    - strings: a list of strings\n",
    "    - substring: a string that we want to search for\n",
    "\n",
    "    Returns:\n",
    "    - a list of strings from the original list that contain the substring\n",
    "    \"\"\"\n",
    "    regex = r\".*\" + re.escape(substring) + r\".*\"\n",
    "    return [string for string in strings if re.match(regex, string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98f80c9-5d13-4b9f-b9ac-b07ceb40de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TextProcessor()\n",
    "transcriptions = tp.process_text(transcriptions_paths)\n",
    "transcriptions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f7aa4-a0a3-416e-9275-be23adfcd7ed",
   "metadata": {},
   "source": [
    "The Radford et al. paper describes a prediction process that begins with a special token. The language being spoken is then predicted based on a unique token for each language in the training set. If there is no speech in an audio segment, the model predicts a token indicating this. The next token specifies whether the task is transcription or translation. Timestamps can also be predicted by including a special token. Finally, the output begins with another special token. For our case, we'll only use the `<|startoftranscript|>` and `<|endoftranscript|>` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e419c3-d4cf-48ad-9b4e-0406fc53e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_start_token(input_string):\n",
    "    return input_string.replace(\"<s>\", \"\")\n",
    "\n",
    "def add_end_token(input_string):\n",
    "    return input_string.replace(\"< s>\", \"\")\n",
    "\n",
    "transcriptions_with_tokens = [\n",
    "    add_end_token(\n",
    "        add_start_token(\n",
    "            transcriptions[idx])) for idx in range(len(transcriptions))\n",
    "]\n",
    "\n",
    "transcriptions_with_tokens = [\n",
    "    transcriptions_with_tokens[i].strip() for i in range(len(transcriptions_with_tokens))\n",
    "]\n",
    "transcriptions_with_tokens[20:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f8844-b1c7-4b8f-bcd4-fade4938ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove text in angled brackets\n",
    "files = []\n",
    "for idx in range(len(transcriptions_with_tokens)):\n",
    "    files.append(\n",
    "        remove_bracketed_text(transcriptions_with_tokens[idx])\n",
    "    )\n",
    "\n",
    "cleaned_files = remove_extra_whitespace(files)\n",
    "cleaned_files[20:24]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c723a98-3718-478f-864c-9f5d142868e4",
   "metadata": {},
   "source": [
    "Finally, we'll pair each transcript with its corresponding filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73873011-ffc5-4db2-8d4d-28bb6be190f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "substrings = []\n",
    "for file_path in transcriptions_paths:\n",
    "    # Split the file path into a list of strings using the \"\\\\\" separator\n",
    "    split_path = file_path.split(\"\\\\\")\n",
    "    # Get the last element of the split path (i.e. the filename)\n",
    "    filename = split_path[-1]\n",
    "    # Remove the \".wav\" extension from the filename\n",
    "    filename = filename.replace(\".txt\", \".wav\")\n",
    "    # Add the extracted filename to the list of filenames\n",
    "    substrings.append(filename)\n",
    "    \n",
    "\n",
    "files = []\n",
    "for idx in range(len(substrings)):\n",
    "    files.append(\n",
    "        find_strings_with_substring(kenspeech_wav_files, substrings[idx])\n",
    "    )\n",
    "    \n",
    "kenspeech_data = []\n",
    "for audio_path, transcript in zip(files, cleaned_files):\n",
    "        kenspeech_data.append((audio_path[0], transcript))\n",
    "\n",
    "kenspeech_data[20:24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d5c91-2369-4a92-af64-94a6a605790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kenspeech_data = pd.DataFrame(kenspeech_data, columns=['file_name', 'transcription'])\n",
    "kenspeech_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d96b170-84d5-4ade-a1eb-6d8e7f0fb847",
   "metadata": {},
   "outputs": [],
   "source": [
    "kenspeech_data.to_csv(\"datasets/swahili/kenspeech_cleaned.txt\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a624dba-9c7e-4508-8a41-99a11244a954",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Babel Swahili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0260f4-d339-44a6-bd14-dc48d873bd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CN = CfgNode()\n",
    "CN.DATASET_DIR = r\"C:\\Users\\Hedronstone\\Desktop\\whisper_event\\datasets\\swahili\\babel_swahili_language_pack\"\n",
    "CN.SEED = 3407\n",
    "CN.DEVICE = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed_everything(CN.SEED, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8618e08d-12d1-49eb-b676-d952def74e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = find_wav_files(r\"C:\\Users\\Hedronstone\\Desktop\\whisper_event\\datasets\\swahili\\babel_swahili_language_pack\")\n",
    "text_paths = find_wav_files(r\"C:\\Users\\Hedronstone\\Desktop\\whisper_event\\datasets\\swahili\\babel_swahili_language_pack\", \".txt\")\n",
    "\n",
    "audio_files = extract_filenames(audio_paths)\n",
    "\n",
    "#collect the audio filenames from our dataset\n",
    "audio_files[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567160d4-9e2c-4b28-8712-eca0f0895127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will seperate out transcriptions without accompanying audio files\n",
    "transcriptions_paths = []\n",
    "\n",
    "for file in audio_files[0]:\n",
    "    transcriptions_paths.append(find_strings_with_substring(text_paths, file)[0])\n",
    "\n",
    "# collect valid transcription paths\n",
    "transcriptions_paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac3ad7-cfc8-4c93-b2e1-6b952a2e0598",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_paths = [transcriptions_paths[i].replace(\".wav\", \".txt\") for i in range(len(transcriptions_paths))]\n",
    "text_paths[:4], audio_paths[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc85371-c306-423f-ba3d-ffd208950dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps, utterances, groups = [], [], []\n",
    "\n",
    "for idx in range(len(transcriptions_paths)):\n",
    "\n",
    "    with open(transcriptions_paths[idx], \"r\") as f:\n",
    "        text_file = f.readlines()\n",
    "\n",
    "    for element in text_file:\n",
    "        if element.startswith('['):\n",
    "            timestamp = element[1:-2]\n",
    "            timestamps.append(float(timestamp))\n",
    "        else:\n",
    "            utterances.append(element[:-1])\n",
    "\n",
    "    utterances = ' '.join(utterances)    \n",
    "    groups.append((str(audio_paths[idx]), utterances))\n",
    "    timestamps = []\n",
    "    utterances = []\n",
    "    \n",
    "groups[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a161e5e-e30d-471c-8bb7-165797cc1f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "babel_data = []\n",
    "\n",
    "for idx in range(len(groups)):\n",
    "    brackets_removed = remove_bracketed_text(groups[idx][1])\n",
    "    whitespace_removed = remove_extra_whitespace([brackets_removed])\n",
    "    parentheses_removed = remove_parentheses(whitespace_removed)\n",
    "    babel_data.append((groups[idx][0], parentheses_removed[0]))\n",
    "\n",
    "print(len(babel_data))\n",
    "babel_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca9f42-7e00-4bec-b7fb-a47a1afc5585",
   "metadata": {},
   "outputs": [],
   "source": [
    "babel_data = pd.DataFrame(babel_data, columns=['file_name', 'transcription'])\n",
    "babel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7079ef4-4499-421b-ba79-3319c502cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "babel_data.to_csv(\"datasets/swahili/babel_cleaned.txt\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b232ac-8660-4a02-b5bd-d39e3d11cfb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Broadcast News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4024614-68bc-476b-a1e3-0fc21ca10fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audio_paths = find_wav_files(r\"C:\\Users\\Hedronstone\\Desktop\\whisper_event\\datasets\\swahili\\data_broadcastnews_sw\", \".wav\")\n",
    "text_paths = find_wav_files(r\"C:\\Users\\Hedronstone\\Desktop\\whisper_event\\datasets\\swahili\\data_broadcastnews_sw\", \".txt\")\n",
    "\n",
    "audio_files = extract_filenames(audio_paths[0])\n",
    "\n",
    "transcriptions_paths = []\n",
    "\n",
    "path = r\"C:\\Users\\Hedronstone\\Desktop\\whisper_event\\datasets\\swahili\\data_broadcastnews_sw\\data\\train\\train_text.txt\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    text_file = f.readlines()\n",
    "\n",
    "processed_lines = process_lines(text_file)\n",
    "\n",
    "broadcastnews_train_data = []\n",
    "\n",
    "for idx in tqdm(range(len(processed_lines))):\n",
    "    a_path = find_strings_with_substring(audio_paths, processed_lines[idx][0])[0]\n",
    "    t_path = processed_lines[idx][1]\n",
    "    broadcastnews_train_data.append([a_path, t_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b39a28-49c7-4f2c-ab6b-9cf19564cb3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Hedronstone\\Desktop\\whisper_event\\datasets\\swahili\\data_broadcastnews_sw\\data\\test\\test_text.txt\"\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    text_file = f.readlines()\n",
    "\n",
    "processed_lines = process_lines(text_file)\n",
    "\n",
    "broadcastnews_test_data = []\n",
    "\n",
    "for idx in tqdm(range(len(processed_lines))):\n",
    "    a_path = find_strings_with_substring(audio_paths, processed_lines[idx][0])[0]\n",
    "    t_path = processed_lines[idx][1]\n",
    "    broadcastnews_test_data.append([a_path, t_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5c81b3-6214-4f41-b638-25f38733ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastnews_train = pd.DataFrame(broadcastnews_train_data, columns=['file_name', 'transcription'])\n",
    "broadcastnews_test = pd.DataFrame(broadcastnews_test_data, columns=['file_name', 'transcription'])\n",
    "\n",
    "broadcastnews_combined = pd.concat([\n",
    "    broadcastnews_train, broadcastnews_test\n",
    "    ]\n",
    ")\n",
    "\n",
    "broadcastnews_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f88223-8104-4159-8b6e-5e6ba0f8ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastnews_combined.to_csv('datasets/swahili/broadcastnews_cleaned.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0c12e-75f8-487f-9e03-a94224acb06c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c207f036-0fda-4003-abaf-d1ccc0e6b215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wave\n",
    "import os\n",
    "\n",
    "def get_total_length_of_audio(audio_file_paths):\n",
    "  total_length = 0\n",
    "  for path in audio_file_paths:\n",
    "    if path.endswith(\".wav\"):\n",
    "      file_size = os.path.getsize(path)\n",
    "      with wave.open(path, 'rb') as audio_file:\n",
    "        total_length += file_size / (audio_file.getnchannels() * audio_file.getsampwidth()) / audio_file.getframerate()\n",
    "  return total_length\n",
    "\n",
    "\n",
    "broadcastnews_total_minuntes = get_total_length_of_audio(\n",
    "    broadcastnews_combined.file_name\n",
    ") / 60\n",
    "\n",
    "\n",
    "kenspeech_total_minuntes = get_total_length_of_audio(\n",
    "    kenspeech_data.file_name\n",
    ") / 60\n",
    "\n",
    "\n",
    "babel_total_minuntes = get_total_length_of_audio(\n",
    "    babel_data.file_name\n",
    ") / 60 \n",
    "\n",
    "print(f\"KenSpeech Total Audio: {kenspeech_total_minuntes / 60}\")\n",
    "print(f\"Babel Total Audio: {babel_total_minuntes / 60}\")\n",
    "print(f\"Broadcast News Total Audio: {broadcastnews_total_minuntes/ 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562f1abc-0389-428b-8920-c7c21b5b967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kenspeech_and_babel_data = pd.concat([kenspeech_data, babel_data])\n",
    "combined_dataset = pd.concat([kenspeech_and_babel_data, broadcastnews_combined])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf7d60a-4832-4b91-8b44-052e5f9678b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_total_minuntes = get_total_length_of_audio(\n",
    "    combined_dataset.file_name\n",
    ") / 60\n",
    "\n",
    "print(combined_total_minuntes / 60)\n",
    "\n",
    "combined_dataset.to_csv('datasets/swahili/metadata.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b7c383-b4f9-479f-b199-4208c5569e7d",
   "metadata": {},
   "source": [
    "## Test Data Loading Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f64e89-fbde-4929-8753-298f609770ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"audiofolder\", data_dir=\"datasets/swahili\", drop_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50011fa-e061-4305-81b1-0d67b9b7346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665df0c6-5868-41d7-9ae6-88b361b9caba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ff822-f2e6-4e01-abe0-efca6b107643",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
