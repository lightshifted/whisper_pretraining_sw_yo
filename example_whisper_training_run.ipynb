{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af02b1a0-6bdb-46b9-869e-c29bb4a47cd8",
   "metadata": {},
   "source": [
    "# Fine-Tune Whisper for Speech-to-Text Using Custom Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab4a8e-38b8-4e80-925a-5619710e99f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325fb3e-138d-43df-9c87-2b8b8bab8119",
   "metadata": {},
   "source": [
    "In this blog post, we will show you how to fine-tune the open-source Whisper speech-to-text model using your own custom datasets. By doing so, you can create a model that is tailored to your specific needs and can achieve higher levels of accuracy and performance. We will guide you through the process of preparing your data, training the model, and evaluating its performance. By the end of this tutorial, you will have a custom Whisper model that is optimized for your unique application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305858a6-756a-4f85-81ca-7bef784a8d40",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b2d94-261b-4c1d-9431-c310e04b7052",
   "metadata": {},
   "source": [
    "To begin, we will first need to import the necessary libraries and modules that we will use throughout this tutorial. This includes the Whisper library, as well as other popular machine learning and data manipulation libraries such as PyTorch and NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b5ecc74-0b1e-4dc9-92c4-e1cd999e38ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d384d343-d2b2-4150-b5c1-74d6945d7b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import WhisperProcessor\n",
    "from datasets import Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867b6e4-3f65-47b1-8d24-081e30a2810b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4fd91-6298-41bc-affd-ab3e25121c43",
   "metadata": {},
   "source": [
    "Next, we will load our custom dataset. This dataset should consist of audio recordings and their corresponding transcriptions. The audio recordings should be in a format that is compatible with Whisper, such as WAV or MP3. The transcriptions should be provided as text files, with one transcription per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a86efe9c-768e-43ed-b799-87c9752484e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>transcription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>datasets/kenspeech/audios/male/speaker_17/twee...</td>\n",
       "      <td>mungu ndiye muumba na mlinzi wa ulimwengu unae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>datasets/kenspeech/audios/male/speaker_17/twee...</td>\n",
       "      <td>hongera ni heshima kubwa kufanya kazi na wewe ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>datasets/kenspeech/audios/male/speaker_17/twee...</td>\n",
       "      <td>asilimia kubwa ya mwanamke anapenda mwanamume ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  \\\n",
       "0  datasets/kenspeech/audios/male/speaker_17/twee...   \n",
       "1  datasets/kenspeech/audios/male/speaker_17/twee...   \n",
       "2  datasets/kenspeech/audios/male/speaker_17/twee...   \n",
       "\n",
       "                                       transcription  \n",
       "0  mungu ndiye muumba na mlinzi wa ulimwengu unae...  \n",
       "1  hongera ni heshima kubwa kufanya kazi na wewe ...  \n",
       "2  asilimia kubwa ya mwanamke anapenda mwanamume ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"datasets/metadata.csv\"\n",
    "combined_dataset = pd.read_csv(data_path)\n",
    "combined_dataset[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f35f1896-2a9e-43da-b968-22f12a5ec499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import whisper\n",
    "from transformers import WhisperFeatureExtractor\n",
    "from transformers import WhisperTokenizer\n",
    "from transformers import WhisperProcessor\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-medium\")\n",
    "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-medium\", language=\"Swahili\", task=\"transcribe\")\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\", language=\"Swahili\", task=\"transcribe\")\n",
    "normalizer = BasicTextNormalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f1996cd-63a8-41b5-850c-32fe0147fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import io\n",
    "import torchaudio\n",
    "import torchaudio.transforms as at\n",
    "    \n",
    "def load_wave(wave_path, sample_rate:int) -> torch.Tensor:\n",
    "    waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "    if sample_rate != sr:\n",
    "        waveform = at.Resample(sr, sample_rate)(waveform)\n",
    "    return waveform\n",
    "\n",
    "def filter_long_samples(dataset, max_length: int=30.0):\n",
    "    indices_of_long_samples = []\n",
    "    for idx, audio_path in enumerate(dataset.file_name):\n",
    "        audio = load_wave(dataset.file_name[idx], sample_rate=16000)\n",
    "        audio_length = audio.size(1) / 16000\n",
    "        if audio_length > max_length:\n",
    "            indices_of_long_samples.append(idx)\n",
    "    return indices_of_long_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa655a4c-e95c-4b5f-99ec-5099c5febb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_of_long_samples = filter_long_samples(combined_dataset)\n",
    "combined_dataset = combined_dataset.drop(indices_of_long_samples).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e59b3dc0-4250-4eb9-8092-97d21014648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareSpeechDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, audio_info_csv, sample_rate, processor, feature_extractor) -> None:\n",
    "        super().__init__()\n",
    "        self.audio_info_csv = audio_info_csv\n",
    "        self.sample_rate = sample_rate\n",
    "        self.processor = processor\n",
    "        self.feature_extractor = feature_extractor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_info_csv)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        audio_path = self.audio_info_csv.file_name[id]\n",
    "        transcription = self.audio_info_csv.transcription[id]\n",
    "        \n",
    "        audio = self._load_wave(audio_path, sample_rate=self.sample_rate)\n",
    "        audio_length = audio.size(1) / self.sample_rate\n",
    "        \n",
    "        # fileter audio_length > 30s\n",
    "\n",
    "        return self.prepare_dataset(\n",
    "            {\n",
    "            'audio' : {'path': audio_path,\n",
    "                       'array': audio,\n",
    "                       'sampling_rate' : self.sample_rate,\n",
    "                       'input_length' : audio.size(1) / self.sample_rate\n",
    "                      },\n",
    "            'sentence' : transcription\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def _load_wave(self, wave_path, sample_rate:int) -> torch.Tensor:\n",
    "        waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "        if sample_rate != sr:\n",
    "            waveform = at.Resample(sr, self.sample_rate)(waveform)\n",
    "        return waveform\n",
    "\n",
    "    def prepare_dataset(self, batch):\n",
    "        # load and resample audio data from 48 to 16kHz\n",
    "        audio = batch[\"audio\"]\n",
    "\n",
    "        # compute log-Mel input features from input audio array \n",
    "        batch[\"input_features\"] = self.feature_extractor(audio[\"array\"].flatten(), sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "        \n",
    "\n",
    "        # encode target text to label ids \n",
    "        batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "        return batch\n",
    "    \n",
    "    def get_output_dictionary(self, dic, split=\"train\"):\n",
    "        return {\n",
    "            f\"{split}\" : dic\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f8736e-180f-48e8-ab85-9ab724dd149b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': 'datasets/kenspeech/audios/male/speaker_17/tweet_286.wav',\n",
       "  'array': tensor([[0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       "  'sampling_rate': 16000,\n",
       "  'input_length': 12.61925},\n",
       " 'sentence': 'mungu ndiye muumba na mlinzi wa ulimwengu unaeza kulindwa na majesshi lakini huezi kukiepuka kifo usalama wetu umehakikishiwa na mwenyezi mungu',\n",
       " 'input_features': array([[-0.7177355, -0.7177355, -0.7177355, ..., -0.7177355, -0.7177355,\n",
       "         -0.7177355],\n",
       "        [-0.7177355, -0.7177355, -0.7177355, ..., -0.7177355, -0.7177355,\n",
       "         -0.7177355],\n",
       "        [-0.7177355, -0.7177355, -0.7177355, ..., -0.7177355, -0.7177355,\n",
       "         -0.7177355],\n",
       "        ...,\n",
       "        [-0.7177355, -0.7177355, -0.7177355, ..., -0.7177355, -0.7177355,\n",
       "         -0.7177355],\n",
       "        [-0.7177355, -0.7177355, -0.7177355, ..., -0.7177355, -0.7177355,\n",
       "         -0.7177355],\n",
       "        [-0.7177355, -0.7177355, -0.7177355, ..., -0.7177355, -0.7177355,\n",
       "         -0.7177355]], dtype=float32),\n",
       " 'labels': [50258,\n",
       "  50318,\n",
       "  50359,\n",
       "  50363,\n",
       "  76,\n",
       "  1063,\n",
       "  84,\n",
       "  220,\n",
       "  273,\n",
       "  30306,\n",
       "  2992,\n",
       "  49353,\n",
       "  1667,\n",
       "  275,\n",
       "  5045,\n",
       "  3992,\n",
       "  5406,\n",
       "  344,\n",
       "  4197,\n",
       "  86,\n",
       "  1501,\n",
       "  84,\n",
       "  2002,\n",
       "  28604,\n",
       "  27576,\n",
       "  471,\n",
       "  4151,\n",
       "  1667,\n",
       "  13673,\n",
       "  442,\n",
       "  4954,\n",
       "  287,\n",
       "  514,\n",
       "  3812,\n",
       "  24967,\n",
       "  3992,\n",
       "  350,\n",
       "  2034,\n",
       "  414,\n",
       "  79,\n",
       "  13599,\n",
       "  350,\n",
       "  351,\n",
       "  78,\n",
       "  505,\n",
       "  304,\n",
       "  2404,\n",
       "  6630,\n",
       "  84,\n",
       "  1105,\n",
       "  13301,\n",
       "  514,\n",
       "  1035,\n",
       "  27537,\n",
       "  4151,\n",
       "  1667,\n",
       "  275,\n",
       "  15615,\n",
       "  1200,\n",
       "  3992,\n",
       "  275,\n",
       "  1063,\n",
       "  84,\n",
       "  50257]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train dataset\n",
    "train_dataset = combined_dataset.reset_index()\n",
    "train_dataset = PrepareSpeechDataset(train_dataset, sample_rate=16000, processor=processor, feature_extractor=feature_extractor)\n",
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4a5e0fc-3b2e-4acb-a440-9f0d897ed2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset common_voice_11_0 (/home/ubuntu/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/sw/11.0.0/f8e47235d9b4e68fa24ed71d63266a02018ccf7194b2a8c9c598a5f3ab304d9f)\n",
      "Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/mozilla-foundation___common_voice_11_0/sw/11.0.0/f8e47235d9b4e68fa24ed71d63266a02018ccf7194b2a8c9c598a5f3ab304d9f/cache-ae4ad253761f21fe.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'audio': {'path': '/home/ubuntu/.cache/huggingface/datasets/downloads/extracted/089cf2c8ac1b9b618eb8b26166acfdd2eb9872a1e2fc8ec769c3b65c985809e0/common_voice_sw_31428161.mp3',\n",
       "  'array': array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       "  'sampling_rate': 48000},\n",
       " 'sentence': 'wachambuzi wa soka wanamtaja messi kama nyota hatari zaidi duniani'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test dataset\n",
    "\n",
    "def normalize_transcriptions(batch):\n",
    "    # optional pre-processing steps\n",
    "    transcription = batch[\"sentence\"]\n",
    "    if do_lower_case:\n",
    "        transcription = transcription.lower()\n",
    "    if do_remove_punctuation:\n",
    "        transcription = normalizer(transcription).strip()\n",
    "    batch[\"sentence\"] = transcription\n",
    "    return batch\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "do_lower_case = True\n",
    "do_remove_punctuation = True\n",
    "\n",
    "common_voice = DatasetDict()\n",
    "common_voice = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"sw\", split=\"test\", use_auth_token=True)\n",
    "\n",
    "common_voice = common_voice.map(\n",
    "    normalize_transcriptions,\n",
    "    remove_columns=[\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"]\n",
    ")\n",
    "\n",
    "next(iter(common_voice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cef83b93-10a9-46e3-9832-057cdf1e390e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5648ead4134aafa44785288bd45c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10238 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Audio\n",
    "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "def prepare_dataset(batch):\n",
    "    # load and resample audio data from 48 to 16kHz\n",
    "    audio = batch[\"audio\"]\n",
    "\n",
    "    # compute log-Mel input features from input audio array \n",
    "    batch[\"input_features\"] = feature_extractor(audio[\"array\"].flatten(), sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "\n",
    "\n",
    "    # encode target text to label ids \n",
    "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "common_vocie = common_voice.map(prepare_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41b58003-f95b-4612-8460-e58d85aaacb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_voice = common_vocie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf64a7a-4b2d-4b9d-8fee-ab467a2a266f",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c390156b-652a-4f12-9d43-a71ead0d1bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
    "        # first treat the audio inputs by simply returning torch tensors\n",
    "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        # get the tokenized label sequences\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "        # pad the labels to max length\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # if bos token is appended in previous tokenization step,\n",
    "        # cut bos token here as it's append later anyways\n",
    "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec21449f-1958-42e1-a433-0649d7758c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0847407a-fdb3-4ed0-bf68-adf4f65a2611",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03d3095e-392f-4392-b92f-bd898cc057b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"wer\")\n",
    "\n",
    "# evaluate with the 'normalised' WER\n",
    "do_normalize_eval = True\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    if do_normalize_eval:\n",
    "        pred_str = [normalizer(pred) for pred in pred_str]\n",
    "        label_str = [normalizer(label) for label in label_str]\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc05eb17-2a38-4cc4-8d61-6c9b6c8ce097",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a1794ac-e04c-4dbc-a475-a87d3510efcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file whisper-medium-sw/checkpoint-3000/config.json\n",
      "Model config WhisperConfig {\n",
      "  \"_name_or_path\": \"openai/whisper-small\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"WhisperForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"begin_suppress_tokens\": [\n",
      "    220,\n",
      "    50257\n",
      "  ],\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 12,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 50258,\n",
      "  \"dropout\": 0.0,\n",
      "  \"encoder_attention_heads\": 12,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 50257,\n",
      "  \"forced_decoder_ids\": null,\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"max_length\": 448,\n",
      "  \"max_source_positions\": 1500,\n",
      "  \"max_target_positions\": 448,\n",
      "  \"model_type\": \"whisper\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_mel_bins\": 80,\n",
      "  \"pad_token_id\": 50257,\n",
      "  \"scale_embedding\": false,\n",
      "  \"suppress_tokens\": [],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.0.dev0\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 51865\n",
      "}\n",
      "\n",
      "loading weights file whisper-medium-sw/checkpoint-3000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "\n",
      "All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at whisper-medium-sw/checkpoint-3000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"whisper-medium-sw/checkpoint-3000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9395dd47-25c9-4420-87d5-b1294dcc55d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens = []\n",
    "model.config.use_cache = False\n",
    "\n",
    "model.config.dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d9799-045f-49f8-b836-0ba741c5b167",
   "metadata": {},
   "source": [
    "## Define the Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "11239335-f294-470c-9d9f-5ead2b4a37b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "# ==== Configuration ====\n",
    "OUTPUT_DIR = \"tuned_weights/whisper-medium-sw\"\n",
    "BATCH_SIZE = 32\n",
    "GRADIENT_ACCUMULATION_STEPS = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "WARMUP_STEPS = 500\n",
    "MAX_STEPS = 5000\n",
    "EVAL_BATCH_SIZE=8\n",
    "SAVE_STEPS = 1000\n",
    "EVAL_STEPS = 1000\n",
    "LOGGING_STEPS=25\n",
    "# =======================\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    report_to=[\"wandb\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2262d74e-0f73-4983-8af5-c5a387e16dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=common_voice,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor.feature_extractor,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "43da06fb-c0dd-4688-840a-cd44939e2412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in tuned_weights/whisper-medium-sw/config.json\n",
      "Model weights saved in tuned_weights/whisper-medium-sw/pytorch_model.bin\n",
      "Feature extractor saved in tuned_weights/whisper-medium-sw/preprocessor_config.json\n",
      "tokenizer config file saved in tuned_weights/whisper-medium-sw/tokenizer_config.json\n",
      "Special tokens file saved in tuned_weights/whisper-medium-sw/special_tokens_map.json\n",
      "added tokens file saved in tuned_weights/whisper-medium-sw/added_tokens.json\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(training_args.output_dir)\n",
    "processor.save_pretrained(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98d97286-9e5a-462c-b2f2-cd12b609704b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:1dnjniz2) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "868868b88d9744a1bc01715e2e338658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.004 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.202912…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">peach-salad-11</strong>: <a href=\"https://wandb.ai/mldude/whisper-medium-sw-fine_tuned/runs/1dnjniz2\" target=\"_blank\">https://wandb.ai/mldude/whisper-medium-sw-fine_tuned/runs/1dnjniz2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221213_142735-1dnjniz2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:1dnjniz2). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66ea2dd354842229c6c6cd62cfa80e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666815633313187, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/wandb/run-20221213_142842-hsqhukmv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/mldude/whisper-medium-sw-fine_tuned/runs/hsqhukmv\" target=\"_blank\">decent-firebrand-12</a></strong> to <a href=\"https://wandb.ai/mldude/whisper-medium-sw-fine_tuned\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/mldude/whisper-medium-sw-fine_tuned/runs/hsqhukmv?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fbdb868e8b0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# W&B argument tracking\n",
    "config = dict(\n",
    "    output_dir=OUTPUT_DIR,  # your repo name\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # increase by 2x for every 2x decrease in batch size\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    max_steps=MAX_STEPS,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    report_to=[\"wandb\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# start a new W&B training run\n",
    "wandb.init(\n",
    "    project=\"whisper-medium-sw-fine_tuned\", \n",
    "    entity=\"mldude\", \n",
    "    tags=[\"whisper-event\", \"whisper-medium-sw\", \"Swahili\", \"SOTA\"],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ad6902d5-8ee8-4d91-9a27-858f50502fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"https://wandb.ai/mldude/whisper-medium-sw-fine_tuned/runs/hsqhukmv?jupyter=true\" style=\"border:none;width:100%;height:420px;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.jupyter.IFrame at 0x7fbd9efe2580>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 12980\n",
      "  Num Epochs = 25\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 5000\n",
      "  Number of trainable parameters = 241734912\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "The following columns in the training set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: audio, sentence. If audio, sentence are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3268' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3268/5000 9:32:00 < 5:03:20, 0.10 it/s, Epoch 16.09/25]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.383123</td>\n",
       "      <td>43.809884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.458013</td>\n",
       "      <td>40.020884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.007700</td>\n",
       "      <td>0.509322</td>\n",
       "      <td>48.430812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: audio, sentence. If audio, sentence are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10238\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to tuned_weights/whisper-medium-sw/checkpoint-1000\n",
      "Configuration saved in tuned_weights/whisper-medium-sw/checkpoint-1000/config.json\n",
      "Model weights saved in tuned_weights/whisper-medium-sw/checkpoint-1000/pytorch_model.bin\n",
      "Feature extractor saved in tuned_weights/whisper-medium-sw/checkpoint-1000/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: audio, sentence. If audio, sentence are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10238\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to tuned_weights/whisper-medium-sw/checkpoint-2000\n",
      "Configuration saved in tuned_weights/whisper-medium-sw/checkpoint-2000/config.json\n",
      "Model weights saved in tuned_weights/whisper-medium-sw/checkpoint-2000/pytorch_model.bin\n",
      "Feature extractor saved in tuned_weights/whisper-medium-sw/checkpoint-2000/preprocessor_config.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: audio, sentence. If audio, sentence are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10238\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to tuned_weights/whisper-medium-sw/checkpoint-3000\n",
      "Configuration saved in tuned_weights/whisper-medium-sw/checkpoint-3000/config.json\n",
      "Model weights saved in tuned_weights/whisper-medium-sw/checkpoint-3000/pytorch_model.bin\n",
      "Feature extractor saved in tuned_weights/whisper-medium-sw/checkpoint-3000/preprocessor_config.json\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1533\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m         )\n\u001b[0;32m-> 1535\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1536\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1783\u001b[0m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1785\u001b[0;31m                 if (\n\u001b[0m\u001b[1;32m   1786\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1787\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%wandb\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
