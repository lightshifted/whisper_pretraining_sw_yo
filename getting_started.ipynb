{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23cf3a8-bd3f-478d-bbde-19dd7aeed0a6",
   "metadata": {},
   "source": [
    "# Whisper Fine Tuning Event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c8026-712f-4c55-b230-40e56c0ffe0a",
   "metadata": {},
   "source": [
    "MIT License"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1202c14d-0876-479b-9af5-feee31d3a8f2",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e468247-d71b-4601-a02e-a471ff9d8a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hedronstone\\desktop\\whisper_event\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import IPython.display\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import torchaudio\n",
    "import torchaudio.transforms as at\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# from tqdm.notebook import tqdm   # for colab\n",
    "from tqdm import tqdm              # for jupyter\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from utils import CfgNode\n",
    "\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d701883-921d-4ca7-b9c6-6173f42d72f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 3407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3407"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CN = CfgNode()\n",
    "\n",
    "CN.DATASET_DIR = r\"C:\\Users\\Hedronstone\\Desktop\\whisper_event\\data_broadcastnews_sw\\data\"\n",
    "CN.SAMPLE_RATE = 16000\n",
    "CN.BATCH_SIZE = 2\n",
    "CN.TRAIN_RATE = 0.8\n",
    "CN.BATCH_SIZE = 2\n",
    "\n",
    "CN.AUDIO_MAX_LENGTH = 480000\n",
    "CN.TEXT_MAX_LENGTH = 120\n",
    "CN.SEED = 3407\n",
    "CN.DEVICE = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed_everything(CN.SEED, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c12f72-cf61-4bbc-8691-e11b7580bc42",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2401f5-5dae-4717-9ca8-0815a7453eab",
   "metadata": {},
   "source": [
    "To make the code efficient, we use the `concurrent.futures` module for multithreading or multiprocessing to parallelize the resampling of the waveform. This can be useful if the resampling operation is computationally expensive and there are multiple waveforms to be resampled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad79880d-b457-4379-920f-684a1de8b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def load_wave(wave_path: str, sample_rate: int=16000) -> torch.Tensor:\n",
    "    \"\"\"Takes a path to a wave file and an optional sample rate as inputs, \n",
    "    loads the waveform and normalizes it, and then resamples the waveform \n",
    "    to the specified sample rate if necessary.\n",
    "    \n",
    "    Arguments:\n",
    "    wave_path -- The path of the audio file.\n",
    "    sample_rate -- The number of individual sound samples per second.\n",
    "    \n",
    "    Returns:\n",
    "    The waveform as a torch.Tensor object.    \n",
    "    \"\"\"\n",
    "    # Load the waveform and resample it if necessary\n",
    "    waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "    if sample_rate != sr:\n",
    "        # Use a thread pool to parallelize the resampling operation\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            waveform = executor.submit(at.Resample(sr, sample_rate), waveform).result()\n",
    "    \n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8de18b8f-7836-4e1e-9aef-adb6b48d4f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wave(wave_path, sample_rate:int=16000) -> torch.Tensor:\n",
    "    waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "    if sample_rate != sr:\n",
    "        waveform = at.Resample(sr, sample_rate)(waveform)\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a722a2f9-6ee0-49b2-91b1-3f2a9428b3d4",
   "metadata": {},
   "source": [
    "### Load Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813ec2f9-387e-45fa-9096-91705b799cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "\n",
    "def get_audio_path(audio_id: str, set_name: str, cfg: CfgNode) -> str:\n",
    "    \"\"\"Returns the path to the audio file with the given ID and set name.\n",
    "\n",
    "    Arguments:\n",
    "    audio_id -- The ID of the audio file.\n",
    "    set_name -- The name of the set (train or test).\n",
    "    cfg -- The configuration node containing the dataset directory.\n",
    "\n",
    "    Returns:\n",
    "    The path to the audio file.\n",
    "    \"\"\"\n",
    "    if set_name == \"test\":\n",
    "        audio_dir = Path(CN.DATASET_DIR) / set_name / \"wav5\"\n",
    "        folders = [d.name for d in audio_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "        for folder in folders:\n",
    "            audio_dirs = Path(CN.DATASET_DIR) / set_name / \"wav5\" / folder\n",
    "            audio_paths = [p.name for p in audio_dirs.glob(\"*.wav\")]\n",
    "\n",
    "            for audio_path in audio_paths:\n",
    "                return(audio_dirs / audio_path)      \n",
    "        \n",
    "    if set_name == \"train\":\n",
    "        audio_dir = Path(cfg.DATASET_DIR) / set_name / \"wav\"\n",
    "        folders = [d.name for d in audio_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "        for folder in folders:\n",
    "            audio_dirs = audio_dir / folder\n",
    "            audio_paths = [p.name for p in audio_dirs.glob(\"*.wav\")]\n",
    "\n",
    "            for audio_path in audio_paths:\n",
    "                return(audio_dirs / audio_path) \n",
    "\n",
    "\n",
    "def stage_audio_data(cfg: CfgNode, set_name: str=\"train\") -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Returns a list of tuples containing audio file IDs, paths, and transcriptions.\n",
    "\n",
    "    Arguments:\n",
    "    cfg -- The configuration node containing the dataset directory.\n",
    "    set_name -- The name of the set (train or test). Defaults to \"train\".\n",
    "\n",
    "    Returns:\n",
    "    A list of tuples containing audio file IDs, paths, and transcriptions.\n",
    "    \"\"\"        \n",
    "    path = Path(CN.DATASET_DIR)\n",
    "    text_path = path / set_name / (set_name + \"_text.txt\")\n",
    "    \n",
    "    with open(text_path, \"r\") as f:\n",
    "        text_list = f.readlines()\n",
    "    \n",
    "    audio_transcript_pairs = []\n",
    "    for text in tqdm(text_list):\n",
    "        audio_id, transcription = text.split(\"=\")\n",
    "        transcription = transcription.replace(\"\\n\", \"\")\n",
    "        audio_path = get_audio_path(audio_id, set_name, CN)\n",
    "        audio_transcript_pairs.append((audio_id, str(audio_path), transcription))\n",
    "    \n",
    "    return audio_transcript_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7f1c478-e840-47e6-957c-a664671adae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████| 10180/10180 [00:09<00:00, 1067.50it/s]\n",
      "100%|█████████████████████| 1991/1991 [00:01<00:00, 1809.37it/s]\n"
     ]
    }
   ],
   "source": [
    "train_audio_transcript_pairs = stage_audio_data(CN, \"train\")\n",
    "test_audio_transcript_pairs = stage_audio_data(CN, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12433299-61fe-4bef-9f7f-fba3db0614f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN AUDIO DATASET NUM:  10180\n",
      "EVAL AUDIO DATASET NUM:  1991\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN AUDIO DATASET NUM: \", len(train_audio_transcript_pairs))\n",
    "print(\"EVAL AUDIO DATASET NUM: \", len(test_audio_transcript_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5837702-c456-4acf-a838-119f67f7a07c",
   "metadata": {},
   "source": [
    "Let's check our `stage_audio_data()` function for compatibility with `load_wave()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c24f8c56-84a8-4a7f-b4d5-098282ee3d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0304, 0.0286, 0.0277,  ..., 0.0885, 0.0946, 0.0796]])\n",
      "tensor([[0.0019, 0.0025, 0.0040,  ..., 0.0955, 0.1230, 0.0723]])\n"
     ]
    }
   ],
   "source": [
    "print(load_wave(train_audio_transcript_pairs[0][1]))\n",
    "print(load_wave(test_audio_transcript_pairs[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d4d84-8135-4aea-b0e7-26dd4e03d62d",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4926e76a-b0c6-4ce3-8995-3c530c82b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "woptions = whisper.DecodingOptions(language=\"sw\", without_timestamps=True)\n",
    "wmodel = whisper.load_model(\"base\")\n",
    "wtokenizer = whisper.tokenizer.get_tokenizer(True, language=\"sw\", task=woptions.task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ef211-5bf4-427a-b750-12f3918390f7",
   "metadata": {},
   "source": [
    "The `SwahiliSpeechDataset` class below creates a dataset of audio information, \n",
    "including the audio file path, associated text, and tokenized text. It \n",
    "uses the sample rate and tokenizer specified in the init method, and has \n",
    "methods to return the length of the dataset and retrieve items from the \n",
    "dataset by index. Each dataset item is a dictionary containing the audio \n",
    "data as input_ids, the tokenized text as labels, and the original text as \n",
    "dec_input_ids.\n",
    "\n",
    "In the `getitem` method, the model retrieves the audio information for a specific index, loads the audio file, pads or trims it to a specific length, and then extracts the log Mel-spectrogram as input_ids. It also encodes the text transcript using the tokenizer, and creates the labels and dec_input_ids for the model's training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eee5537-d171-4dbf-aec8-52d85343a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwahiliSpeechDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Extracts log Mel-spectrogram as input_ids, encodes text transcripts using tokenizer, and creates the\n",
    "    labels and dec_input_ids for the model's training.\n",
    "    \n",
    "    Arguments:\n",
    "        audio_info_list -- A list of audio information, including the audio ID, audio path, and text transcript.\n",
    "        sample_rate -- The sample rate of the audio files, which defaults to 16e3 (16kHz).\n",
    "        tokenizer -- An instance of the whisper.tokenizer class that is used to encode the text transcript.        \n",
    "    \"\"\"\n",
    "    def __init__(self, audio_info_list: str, tokenizer: whisper.tokenizer, sample_rate: int=16e3) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.audio_info_list = audio_info_list\n",
    "        self.sample_rate = sample_rate\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_info_list)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        audio_id, audio_path, text = self.audio_info_list[id]\n",
    "        \n",
    "        audio = load_wave(audio_path, sample_rate=self.sample_rate)\n",
    "        audio = whisper.pad_or_trim(audio.flatten())\n",
    "        mel = whisper.log_mel_spectrogram(audio)\n",
    "        \n",
    "        text = self.audio_info_list[id][2]\n",
    "        text = [*self.tokenizer.sot_sequence_including_notimestamps] + self.tokenizer.encode(text)\n",
    "        labels = text[1:] + [self.tokenizer.eot]\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": mel,\n",
    "            \"labels\": labels,\n",
    "            \"dec_input_ids\": text\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b162f5-33cb-4016-87f7-7101fb314dca",
   "metadata": {},
   "source": [
    "When called with a list of features, `WhisperDataCollatorWhithPadding` class \n",
    "collects the `input_ids` and `labels` attributes from each feature and concatenates \n",
    "them into a single tensor. It then pads the `labels` and `dec_input_ids` attributes \n",
    "with the constant values -100 and 50257, respectively, to the maximum length of all \n",
    "values in these attributes.Padded attributes are then converted into tensors and \n",
    "returned as a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33a8a479-afba-4c0a-88d7-264030c63fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperDataCollatorWhithPadding:\n",
    "    \"\"\"Prepares batches for model training.\n",
    "    \n",
    "    Arguments:\n",
    "        input_ids -- A list of input IDs representing the sequence of tokens in the input text.\n",
    "        labels -- A list of labels corresponding to the input sequence.\n",
    "        dec_input_ids -- A list of input IDs representing the sequence of tokens in the decoder input text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_ids = [feature[\"input_ids\"] for feature in features]\n",
    "        labels = [feature[\"labels\"] for feature in features]\n",
    "        dec_input_ids = [feature[\"dec_input_ids\"] for feature in features]\n",
    "\n",
    "        input_ids = torch.concat([input_id[None, :] for input_id in input_ids])\n",
    "\n",
    "        max_lengths = [len(lab) for lab in labels + dec_input_ids]\n",
    "        max_length = max(max_lengths)\n",
    "\n",
    "        labels = [np.pad(lab, (0, max_length - lab_len), 'constant', constant_values=-100) for lab, lab_len in zip(labels, max_lengths)]\n",
    "        dec_input_ids = [np.pad(e, (0, max_length - e_len), 'constant', constant_values=50257) for e, e_len in zip(dec_input_ids, max_lengths)] # 50257 is eot token id\n",
    "\n",
    "        batch = {\n",
    "            \"labels\": labels,\n",
    "            \"dec_input_ids\": dec_input_ids\n",
    "        }\n",
    "\n",
    "        batch = {k: torch.tensor(np.array(v), requires_grad=False) for k, v in batch.items()}\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e00ce6-9684-45d2-a589-d664ec473adb",
   "metadata": {},
   "source": [
    "Let's now check our `SwahiliSpeechDataset` and `WhisperDataCollatorWhithPadding` classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4f058f7-fbfb-4f69-8f18-fdc4666e91e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 37])\n",
      "torch.Size([2, 80, 3000])\n",
      "torch.Size([2, 37])\n",
      "<|sw|><|transcribe|><|notimestamps|>ya redio france internanational mimi ni zuhra mwera<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<|startoftranscript|><|sw|><|transcribe|><|notimestamps|>ya redio france internanational mimi ni zuhra mwera<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<|sw|><|transcribe|><|notimestamps|>marekani yasema iko tayari kuisaidia korea kusini kuikabili korea kaskazini<|endoftext|>\n",
      "<|startoftranscript|><|sw|><|transcribe|><|notimestamps|>marekani yasema iko tayari kuisaidia korea kusini kuikabili korea kaskazini\n"
     ]
    }
   ],
   "source": [
    "dataset = SwahiliSpeechDataset(test_audio_transcript_pairs, wtokenizer, CN.SAMPLE_RATE)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2, collate_fn=WhisperDataCollatorWhithPadding())\n",
    "\n",
    "for b in loader:\n",
    "    print(b[\"labels\"].shape)\n",
    "    print(b[\"input_ids\"].shape)\n",
    "    print(b[\"dec_input_ids\"].shape)\n",
    "    \n",
    "    for token, dec in zip(b[\"labels\"], b[\"dec_input_ids\"]):\n",
    "        token[token == -100] = wtokenizer.eot\n",
    "        text = wtokenizer.decode(token, skip_special_tokens=False)\n",
    "        print(text)\n",
    "\n",
    "        dec[dec == -100] = wtokenizer.eot\n",
    "        text = wtokenizer.decode(dec, skip_special_tokens=False)\n",
    "        print(text)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44f8f10d-eead-435c-9453-ac5cc740dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50258, 50318, 50359, 50363,  3016,  2182,  1004,   431,   719,  2154,\n",
      "           282,  1478,   275, 10121,  3867,  2164,    71,   424,   275,  1554,\n",
      "            64, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "         50257, 50257, 50257, 50257, 50257, 50257, 50257],\n",
      "        [50258, 50318, 50359, 50363, 15455,    74,  3782,   288,   296,  5619,\n",
      "           741,  4093,   220,    83,   320,  3504, 17807,  3837,   327,   654,\n",
      "           350,   418,    64,   350,   301,  3812, 17807,  1035,   455,  2312,\n",
      "           350,   418,    64,   350,  3863,   921,  3812]])\n",
      "torch.Size([2, 80, 3000]) torch.Size([2, 37]) torch.Size([2, 1500, 512])\n",
      "torch.Size([2, 1500, 512])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    audio_features = wmodel.encoder(b[\"input_ids\"].cuda())\n",
    "    input_ids = b[\"input_ids\"]\n",
    "    labels = b[\"labels\"].long()\n",
    "    dec_input_ids = b[\"dec_input_ids\"].long()\n",
    "        \n",
    "    audio_features = wmodel.encoder(input_ids.cuda())\n",
    "    print(dec_input_ids)\n",
    "    print(input_ids.shape, dec_input_ids.shape, audio_features.shape)\n",
    "    print(audio_features.shape)\n",
    "    print()\n",
    "out = wmodel.decoder(dec_input_ids.cuda(), audio_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00abdb88-61d5-46fe-8504-0d56b339b31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 37, 51865])\n",
      "torch.Size([74, 51865])\n",
      "torch.Size([74])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)\n",
    "print(out.view(-1, out.size(-1)).shape)\n",
    "print(b[\"labels\"].view(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4ed18fb-23db-4a96-bb22-1cf558aad0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i radiio frans intes e mimi ni zhra mureraa\n",
      " i diereau- niireakanawuataaereakisaauuaka kuso\n"
     ]
    }
   ],
   "source": [
    "tokens = torch.argmax(out, dim=2)\n",
    "for token in tokens:\n",
    "    token[token == -100] = wtokenizer.eot\n",
    "    text = wtokenizer.decode(token, skip_special_tokens=True)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6232d740-8932-418b-9e5c-a04e8bc03714",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf62db0-66da-4f3f-8b85-b9522f82b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "CN.LEARNING_RATE = 0.0005\n",
    "CN.WEIGHT_DECAY = 0.01\n",
    "CN.ADAM_EPSILON = 1e-8\n",
    "CN.WARMUP_STEPS = 2\n",
    "CN.BATCH_SIZE = 16\n",
    "CN.NUM_WORKER = 2\n",
    "CN.NUM_TRAIN_EPOCHS = 1\n",
    "CN.GRADIENT_ACCUMULATION_STEPS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955d40c-fd6f-48f2-8234-bd62635f7e54",
   "metadata": {},
   "source": [
    "Let's create `WhisperModelModule` class that extends the LightningModule class from the PyTorch Lightning library. The class will initialize a whisper model for our chosen language and train only the decoder part of the model with a given dataset. The class will also define the forward method, training and validation steps, and the configuration of optimizers and schedulers. The class will log metrics such as loss, WER, and CER during training and validation.\n",
    "\n",
    "#### Under Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60252ee8-90ff-4675-b541-c43a295f35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperModelModule(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self):\n",
    "        return\n",
    "    \n",
    "    def training_step(self):\n",
    "        return\n",
    "    \n",
    "    def validation_step(self):\n",
    "        return\n",
    "    \n",
    "    def configure_optimizer(self):\n",
    "        return\n",
    "    \n",
    "    def setup(self):\n",
    "        return\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
