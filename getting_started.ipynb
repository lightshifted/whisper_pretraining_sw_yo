{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a23cf3a8-bd3f-478d-bbde-19dd7aeed0a6",
   "metadata": {},
   "source": [
    "# Whisper Fine Tuning Event"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c8026-712f-4c55-b230-40e56c0ffe0a",
   "metadata": {},
   "source": [
    "MIT License"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1202c14d-0876-479b-9af5-feee31d3a8f2",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e468247-d71b-4601-a02e-a471ff9d8a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import whisper\n",
    "import torchaudio\n",
    "import torchaudio.transforms as at\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# from tqdm.notebook import tqdm   # for colab\n",
    "from tqdm import tqdm              # for jupyter\n",
    "import evaluate\n",
    "\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from utils import CfgNode\n",
    "\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f22650a0-2fdd-49f9-b005-e3e9b340423d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aebd61a99c34d288c3c73215557bb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d701883-921d-4ca7-b9c6-6173f42d72f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 3407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3407"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CN = CfgNode()\n",
    "\n",
    "CN.DATASET_DIR = r\"C:\\Users\\Hedronstone\\Desktop\\whisper_event\\data_broadcastnews_sw\\data\"\n",
    "CN.SAMPLE_RATE = 16000\n",
    "CN.BATCH_SIZE = 2\n",
    "CN.TRAIN_RATE = 0.8\n",
    "CN.BATCH_SIZE = 2\n",
    "\n",
    "CN.AUDIO_MAX_LENGTH = 480000\n",
    "CN.TEXT_MAX_LENGTH = 120\n",
    "CN.SEED = 3407\n",
    "CN.DEVICE = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed_everything(CN.SEED, workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c12f72-cf61-4bbc-8691-e11b7580bc42",
   "metadata": {},
   "source": [
    "## Util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2401f5-5dae-4717-9ca8-0815a7453eab",
   "metadata": {},
   "source": [
    "To make the code efficient, we use the `concurrent.futures` module for multithreading or multiprocessing to parallelize the resampling of the waveform. This can be useful if the resampling operation is computationally expensive and there are multiple waveforms to be resampled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad79880d-b457-4379-920f-684a1de8b4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def load_wave(wave_path: str, sample_rate: int=16000) -> torch.Tensor:\n",
    "    \"\"\"Takes a path to a wave file and an optional sample rate as inputs, \n",
    "    loads the waveform and normalizes it, and then resamples the waveform \n",
    "    to the specified sample rate if necessary.\n",
    "    \n",
    "    Arguments:\n",
    "    wave_path -- The path of the audio file.\n",
    "    sample_rate -- The number of individual sound samples per second.\n",
    "    \n",
    "    Returns:\n",
    "    The waveform as a torch.Tensor object.    \n",
    "    \"\"\"\n",
    "    # Load the waveform and resample it if necessary\n",
    "    waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "    if sample_rate != sr:\n",
    "        # Use a thread pool to parallelize the resampling operation\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            waveform = executor.submit(at.Resample(sr, sample_rate), waveform).result()\n",
    "    \n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de18b8f-7836-4e1e-9aef-adb6b48d4f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wave(wave_path, sample_rate:int=16000) -> torch.Tensor:\n",
    "    waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "    if sample_rate != sr:\n",
    "        waveform = at.Resample(sr, sample_rate)(waveform)\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a722a2f9-6ee0-49b2-91b1-3f2a9428b3d4",
   "metadata": {},
   "source": [
    "### Load Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "813ec2f9-387e-45fa-9096-91705b799cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import List, Tuple\n",
    "\n",
    "def get_audio_path(audio_id: str, set_name: str, cfg: CfgNode) -> str:\n",
    "    \"\"\"Returns the path to the audio file with the given ID and set name.\n",
    "\n",
    "    Arguments:\n",
    "    audio_id -- The ID of the audio file.\n",
    "    set_name -- The name of the set (train or test).\n",
    "    cfg -- The configuration node containing the dataset directory.\n",
    "\n",
    "    Returns:\n",
    "    The path to the audio file.\n",
    "    \"\"\"\n",
    "    if set_name == \"test\":\n",
    "        audio_dir = Path(CN.DATASET_DIR) / set_name / \"wav5\"\n",
    "        folders = [d.name for d in audio_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "        for folder in folders:\n",
    "            audio_dirs = Path(CN.DATASET_DIR) / set_name / \"wav5\" / folder\n",
    "            audio_paths = [p.name for p in audio_dirs.glob(\"*.wav\")]\n",
    "\n",
    "            for audio_path in audio_paths:\n",
    "                return(audio_dirs / audio_path)      \n",
    "        \n",
    "    if set_name == \"train\":\n",
    "        audio_dir = Path(cfg.DATASET_DIR) / set_name / \"wav\"\n",
    "        folders = [d.name for d in audio_dir.iterdir() if d.is_dir()]\n",
    "\n",
    "        for folder in folders:\n",
    "            audio_dirs = audio_dir / folder\n",
    "            audio_paths = [p.name for p in audio_dirs.glob(\"*.wav\")]\n",
    "\n",
    "            for audio_path in audio_paths:\n",
    "                return(audio_dirs / audio_path) \n",
    "\n",
    "\n",
    "def stage_audio_data(cfg: CfgNode, set_name: str=\"train\") -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Returns a list of tuples containing audio file IDs, paths, and transcriptions.\n",
    "\n",
    "    Arguments:\n",
    "    cfg -- The configuration node containing the dataset directory.\n",
    "    set_name -- The name of the set (train or test). Defaults to \"train\".\n",
    "\n",
    "    Returns:\n",
    "    A list of tuples containing audio file IDs, paths, and transcriptions.\n",
    "    \"\"\"        \n",
    "    path = Path(CN.DATASET_DIR)\n",
    "    text_path = path / set_name / (set_name + \"_text.txt\")\n",
    "    \n",
    "    with open(text_path, \"r\") as f:\n",
    "        text_list = f.readlines()\n",
    "    \n",
    "    audio_transcript_pairs = []\n",
    "    for text in tqdm(text_list):\n",
    "        audio_id, transcription = text.split(\"=\")\n",
    "        transcription = transcription.replace(\"\\n\", \"\")\n",
    "        audio_path = get_audio_path(audio_id, set_name, CN)\n",
    "        audio_transcript_pairs.append((audio_id, str(audio_path), transcription))\n",
    "    \n",
    "    return audio_transcript_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7f1c478-e840-47e6-957c-a664671adae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 10180/10180 [00:10<00:00, 1015.42it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████| 1991/1991 [00:01<00:00, 1735.83it/s]\n"
     ]
    }
   ],
   "source": [
    "train_audio_transcript_pairs = stage_audio_data(CN, \"train\")\n",
    "test_audio_transcript_pairs = stage_audio_data(CN, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12433299-61fe-4bef-9f7f-fba3db0614f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN AUDIO DATASET NUM:  10180\n",
      "EVAL AUDIO DATASET NUM:  1991\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN AUDIO DATASET NUM: \", len(train_audio_transcript_pairs))\n",
    "print(\"EVAL AUDIO DATASET NUM: \", len(test_audio_transcript_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5837702-c456-4acf-a838-119f67f7a07c",
   "metadata": {},
   "source": [
    "Let's check our `stage_audio_data()` function for compatibility with `load_wave()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c24f8c56-84a8-4a7f-b4d5-098282ee3d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0304, 0.0286, 0.0277,  ..., 0.0885, 0.0946, 0.0796]])\n",
      "tensor([[0.0019, 0.0025, 0.0040,  ..., 0.0955, 0.1230, 0.0723]])\n"
     ]
    }
   ],
   "source": [
    "print(load_wave(train_audio_transcript_pairs[0][1]))\n",
    "print(load_wave(test_audio_transcript_pairs[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d4d84-8135-4aea-b0e7-26dd4e03d62d",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4926e76a-b0c6-4ce3-8995-3c530c82b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "woptions = whisper.DecodingOptions(language=\"sw\", without_timestamps=True)\n",
    "wmodel = whisper.load_model(\"base\")\n",
    "wtokenizer = whisper.tokenizer.get_tokenizer(True, language=\"sw\", task=woptions.task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634ef211-5bf4-427a-b750-12f3918390f7",
   "metadata": {},
   "source": [
    "The `SwahiliSpeechDataset` class below creates a dataset of audio information, \n",
    "including the audio file path, associated text, and tokenized text. It \n",
    "uses the sample rate and tokenizer specified in the init method, and has \n",
    "methods to return the length of the dataset and retrieve items from the \n",
    "dataset by index. Each dataset item is a dictionary containing the audio \n",
    "data as input_ids, the tokenized text as labels, and the original text as \n",
    "dec_input_ids.\n",
    "\n",
    "In the `getitem` method, the model retrieves the audio information for a specific index, loads the audio file, pads or trims it to a specific length, and then extracts the log Mel-spectrogram as input_ids. It also encodes the text transcript using the tokenizer, and creates the labels and dec_input_ids for the model's training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1eee5537-d171-4dbf-aec8-52d85343a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwahiliSpeechDataset(torch.utils.data.Dataset):\n",
    "    \"\"\" Extracts log Mel-spectrogram as input_ids, encodes text transcripts using tokenizer, and creates the\n",
    "    labels and dec_input_ids for the model's training.\n",
    "    \n",
    "    Arguments:\n",
    "        audio_info_list -- A list of audio information, including the audio ID, audio path, and text transcript.\n",
    "        sample_rate -- The sample rate of the audio files, which defaults to 16e3 (16kHz).\n",
    "        tokenizer -- An instance of the whisper.tokenizer class that is used to encode the text transcript.        \n",
    "    \"\"\"\n",
    "    def __init__(self, audio_info_list: str, tokenizer: whisper.tokenizer, sample_rate: int=16e3) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.audio_info_list = audio_info_list\n",
    "        self.sample_rate = sample_rate\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.audio_info_list)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        audio_id, audio_path, text = self.audio_info_list[id]\n",
    "        \n",
    "        audio = load_wave(audio_path, sample_rate=self.sample_rate)\n",
    "        audio = whisper.pad_or_trim(audio.flatten())\n",
    "        mel = whisper.log_mel_spectrogram(audio)\n",
    "        \n",
    "        text = self.audio_info_list[id][2]\n",
    "        text = [*self.tokenizer.sot_sequence_including_notimestamps] + self.tokenizer.encode(text)\n",
    "        labels = text[1:] + [self.tokenizer.eot]\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": mel,\n",
    "            \"labels\": labels,\n",
    "            \"dec_input_ids\": text\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b162f5-33cb-4016-87f7-7101fb314dca",
   "metadata": {},
   "source": [
    "When called with a list of features, `WhisperDataCollatorWhithPadding` class \n",
    "collects the `input_ids` and `labels` attributes from each feature and concatenates \n",
    "them into a single tensor. It then pads the `labels` and `dec_input_ids` attributes \n",
    "with the constant values -100 and 50257, respectively, to the maximum length of all \n",
    "values in these attributes.Padded attributes are then converted into tensors and \n",
    "returned as a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33a8a479-afba-4c0a-88d7-264030c63fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperDataCollatorWhithPadding:\n",
    "    \"\"\"Prepares batches for model training.\n",
    "    \n",
    "    Arguments:\n",
    "        input_ids -- A list of input IDs representing the sequence of tokens in the input text.\n",
    "        labels -- A list of labels corresponding to the input sequence.\n",
    "        dec_input_ids -- A list of input IDs representing the sequence of tokens in the decoder input text.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_ids = [feature[\"input_ids\"] for feature in features]\n",
    "        labels = [feature[\"labels\"] for feature in features]\n",
    "        dec_input_ids = [feature[\"dec_input_ids\"] for feature in features]\n",
    "\n",
    "        input_ids = torch.concat([input_id[None, :] for input_id in input_ids])\n",
    "\n",
    "        max_lengths = [len(lab) for lab in labels + dec_input_ids]\n",
    "        max_length = max(max_lengths)\n",
    "\n",
    "        labels = [np.pad(lab, (0, max_length - lab_len), 'constant', constant_values=-100) for lab, lab_len in zip(labels, max_lengths)]\n",
    "        dec_input_ids = [np.pad(e, (0, max_length - e_len), 'constant', constant_values=50257) for e, e_len in zip(dec_input_ids, max_lengths)] # 50257 is eot token id\n",
    "\n",
    "        batch = {\n",
    "            \"labels\": labels,\n",
    "            \"dec_input_ids\": dec_input_ids\n",
    "        }\n",
    "\n",
    "        batch = {k: torch.tensor(np.array(v), requires_grad=False) for k, v in batch.items()}\n",
    "        batch[\"input_ids\"] = input_ids\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e00ce6-9684-45d2-a589-d664ec473adb",
   "metadata": {},
   "source": [
    "Let's now check our `SwahiliSpeechDataset` and `WhisperDataCollatorWhithPadding` classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4f058f7-fbfb-4f69-8f18-fdc4666e91e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 37])\n",
      "torch.Size([2, 80, 3000])\n",
      "torch.Size([2, 37])\n",
      "<|sw|><|transcribe|><|notimestamps|>ya redio france internanational mimi ni zuhra mwera<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<|startoftranscript|><|sw|><|transcribe|><|notimestamps|>ya redio france internanational mimi ni zuhra mwera<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "<|sw|><|transcribe|><|notimestamps|>marekani yasema iko tayari kuisaidia korea kusini kuikabili korea kaskazini<|endoftext|>\n",
      "<|startoftranscript|><|sw|><|transcribe|><|notimestamps|>marekani yasema iko tayari kuisaidia korea kusini kuikabili korea kaskazini\n"
     ]
    }
   ],
   "source": [
    "dataset = SwahiliSpeechDataset(test_audio_transcript_pairs, wtokenizer, CN.SAMPLE_RATE)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=2, collate_fn=WhisperDataCollatorWhithPadding())\n",
    "\n",
    "for b in loader:\n",
    "    print(b[\"labels\"].shape)\n",
    "    print(b[\"input_ids\"].shape)\n",
    "    print(b[\"dec_input_ids\"].shape)\n",
    "    \n",
    "    for token, dec in zip(b[\"labels\"], b[\"dec_input_ids\"]):\n",
    "        token[token == -100] = wtokenizer.eot\n",
    "        text = wtokenizer.decode(token, skip_special_tokens=False)\n",
    "        print(text)\n",
    "\n",
    "        dec[dec == -100] = wtokenizer.eot\n",
    "        text = wtokenizer.decode(dec, skip_special_tokens=False)\n",
    "        print(text)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "44f8f10d-eead-435c-9453-ac5cc740dcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50258, 50318, 50359, 50363,  3016,  2182,  1004,   431,   719,  2154,\n",
      "           282,  1478,   275, 10121,  3867,  2164,    71,   424,   275,  1554,\n",
      "            64, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "         50257, 50257, 50257, 50257, 50257, 50257, 50257],\n",
      "        [50258, 50318, 50359, 50363, 15455,    74,  3782,   288,   296,  5619,\n",
      "           741,  4093,   220,    83,   320,  3504, 17807,  3837,   327,   654,\n",
      "           350,   418,    64,   350,   301,  3812, 17807,  1035,   455,  2312,\n",
      "           350,   418,    64,   350,  3863,   921,  3812]])\n",
      "torch.Size([2, 80, 3000]) torch.Size([2, 37]) torch.Size([2, 1500, 512])\n",
      "torch.Size([2, 1500, 512])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    audio_features = wmodel.encoder(b[\"input_ids\"].cuda())\n",
    "    input_ids = b[\"input_ids\"]\n",
    "    labels = b[\"labels\"].long()\n",
    "    dec_input_ids = b[\"dec_input_ids\"].long()\n",
    "        \n",
    "    audio_features = wmodel.encoder(input_ids.cuda())\n",
    "    print(dec_input_ids)\n",
    "    print(input_ids.shape, dec_input_ids.shape, audio_features.shape)\n",
    "    print(audio_features.shape)\n",
    "    print()\n",
    "out = wmodel.decoder(dec_input_ids.cuda(), audio_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00abdb88-61d5-46fe-8504-0d56b339b31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 37, 51865])\n",
      "torch.Size([74, 51865])\n",
      "torch.Size([74])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)\n",
    "print(out.view(-1, out.size(-1)).shape)\n",
    "print(b[\"labels\"].view(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4ed18fb-23db-4a96-bb22-1cf558aad0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i radiio frans intes e mimi ni zhra mureraa\n",
      " i diereau- niireakanawuataaereakisaauuaka kuso\n"
     ]
    }
   ],
   "source": [
    "tokens = torch.argmax(out, dim=2)\n",
    "for token in tokens:\n",
    "    token[token == -100] = wtokenizer.eot\n",
    "    text = wtokenizer.decode(token, skip_special_tokens=True)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548db1a-e804-4d8e-b985-3b11c8f68a16",
   "metadata": {},
   "source": [
    "## Data Preprocess and Synthesis Pipeline\n",
    "We will create a pipeline for processing two data streams. The pipeline will load the data from each stream, preprocess it as needed, and then return it in a format compatible with the Whisper model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c43210a2-6e12-4939-84f3-f5b2a2d7a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio, interleave_datasets, IterableDataset, load_dataset\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb76583-4bd8-4b2c-8150-6f8e68600471",
   "metadata": {},
   "source": [
    "We now create `DatasetLoader` class, which takes in the names and config names of datasets as well as the names of text columns for each dataset. The class has two methods: `load_datasets`, which loads and processes the datasets, and `print_samples`, which prints a specified number of samples from each dataset. The datasets are processed by casting the \"audio\" column to the correct format, renaming the text column to \"sentence\", and removing any other columns. The class can be used to easily load and process multiple datasets with a consistent format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4cddb53b-7f3f-4796-84a4-9390bf25455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetLoader:\n",
    "    def __init__(self, dataset_names, dataset_config_names, text_column_names):\n",
    "        # Initialize class variables\n",
    "        self.dataset_names = dataset_names\n",
    "        self.dataset_config_names = dataset_config_names\n",
    "        self.text_column_names = text_column_names\n",
    "    \n",
    "    def load_datasets(self):\n",
    "        # Create a list of splits for each dataset\n",
    "        self.splits = [\"train\" for i in range(len(self.dataset_names))]\n",
    "        # Initialize empty list to store datasets\n",
    "        datasets = []\n",
    "\n",
    "        # Loop through each dataset and load, process, and append to datasets list\n",
    "        for i, dataset_name in tqdm(enumerate(self.dataset_names)):\n",
    "            dataset = load_dataset(self.dataset_names[i], self.dataset_config_names[i], split=self.splits[i], streaming=True)\n",
    "            dataset = dataset.cast_column(\"audio\", Audio(CN.SAMPLE_RATE))\n",
    "            if self.text_column_names[i] != \"sentence\":\n",
    "                dataset = dataset.rename_column(self.text_column_names[i], \"sentence\")\n",
    "            dataset = dataset.remove_columns(set(dataset.features.keys()) - set([\"audio\", \"sentence\"]))\n",
    "            datasets.append(dataset)\n",
    "        return datasets\n",
    "\n",
    "    def print_samples(self, num_samples):\n",
    "        # Load datasets\n",
    "        datasets = self.load_datasets()\n",
    "        # Loop through each dataset and print specified number of samples\n",
    "        for dataset in datasets:\n",
    "            for i, sample in enumerate(dataset):\n",
    "                print(i, sample[\"sentence\"])\n",
    "                if i == num_samples-1:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9d0a6-7e29-4d82-95d3-c8706ba8b1aa",
   "metadata": {},
   "source": [
    "Create instance of DatasetLoader and print samples from loaded datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5adddb84-073f-49bc-a050-691d1b1e5871",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:03,  1.52s/it]\n",
      "Reading metadata...: 26614it [00:02, 12683.20it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input signal length=0 is too small to resample from 32000->16000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\features\\audio.py:303\u001b[0m, in \u001b[0;36mAudio._decode_mp3\u001b[1;34m(self, path_or_file)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:  \u001b[38;5;66;03m# try torchaudio anyway because sometimes it works (depending on the os and os packages installed)\u001b[39;00m\n\u001b[1;32m--> 303\u001b[0m     array, sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode_mp3_torchaudio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\features\\audio.py:348\u001b[0m, in \u001b[0;36mAudio._decode_mp3_torchaudio\u001b[1;34m(self, path_or_file)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resampler \u001b[38;5;241m=\u001b[39m T\u001b[38;5;241m.\u001b[39mResample(sampling_rate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_rate)\n\u001b[1;32m--> 348\u001b[0m array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_rate\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\torchaudio\\transforms\\_transforms.py:1002\u001b[0m, in \u001b[0;36mResample.forward\u001b[1;34m(self, waveform)\u001b[0m\n\u001b[0;32m   1001\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m waveform\n\u001b[1;32m-> 1002\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_sinc_resample_kernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgcd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\torchaudio\\functional\\functional.py:1532\u001b[0m, in \u001b[0;36m_apply_sinc_resample_kernel\u001b[1;34m(waveform, orig_freq, new_freq, gcd, kernel, width)\u001b[0m\n\u001b[0;32m   1531\u001b[0m shape \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m-> 1532\u001b[0m waveform \u001b[38;5;241m=\u001b[39m \u001b[43mwaveform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1534\u001b[0m num_wavs, length \u001b[38;5;241m=\u001b[39m waveform\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cannot reshape tensor of 0 elements into shape [-1, 0] because the unspecified dimension size -1 can be any value and is ambiguous",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataset_loader \u001b[38;5;241m=\u001b[39m DatasetLoader([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmozilla-foundation/common_voice_11_0\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/fleurs\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msw_ke\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscription\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdataset_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[79], line 28\u001b[0m, in \u001b[0;36mDatasetLoader.print_samples\u001b[1;34m(self, num_samples)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Loop through each dataset and print specified number of samples\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28mprint\u001b[39m(i, sample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m num_samples\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py:747\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 747\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter():\n\u001b[0;32m    748\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[0;32m    749\u001b[0m             \u001b[38;5;66;03m# `IterableDataset` automatically fills missing columns with None.\u001b[39;00m\n\u001b[0;32m    750\u001b[0m             \u001b[38;5;66;03m# This is done with `_apply_feature_types`.\u001b[39;00m\n\u001b[0;32m    751\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m _apply_feature_types(example, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, token_per_repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_per_repo_id)\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py:737\u001b[0m, in \u001b[0;36mIterableDataset._iter\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    736\u001b[0m     ex_iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ex_iterable\n\u001b[1;32m--> 737\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m ex_iterable\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py:415\u001b[0m, in \u001b[0;36mMappedExamplesIterable.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m         current_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;66;03m# If not batched, we can apply the transform and yield the example directly\u001b[39;00m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;66;03m# first copy the example, since we might drop some keys\u001b[39;00m\n\u001b[0;32m    418\u001b[0m         example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(example)\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;66;03m# then apply the transform\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py:653\u001b[0m, in \u001b[0;36mTypedExamplesIterable.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;66;03m# Then for each example, `TypedExamplesIterable` automatically fills missing columns with None.\u001b[39;00m\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;66;03m# This is done with `_apply_feature_types`.\u001b[39;00m\n\u001b[1;32m--> 653\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable:\n\u001b[0;32m    654\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m key, _apply_feature_types(example, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, token_per_repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_per_repo_id)\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py:415\u001b[0m, in \u001b[0;36mMappedExamplesIterable.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m         current_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[0;32m    416\u001b[0m         \u001b[38;5;66;03m# If not batched, we can apply the transform and yield the example directly\u001b[39;00m\n\u001b[0;32m    417\u001b[0m         \u001b[38;5;66;03m# first copy the example, since we might drop some keys\u001b[39;00m\n\u001b[0;32m    418\u001b[0m         example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(example)\n\u001b[0;32m    419\u001b[0m         \u001b[38;5;66;03m# then apply the transform\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py:654\u001b[0m, in \u001b[0;36mTypedExamplesIterable.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    651\u001b[0m     \u001b[38;5;66;03m# Then for each example, `TypedExamplesIterable` automatically fills missing columns with None.\u001b[39;00m\n\u001b[0;32m    652\u001b[0m     \u001b[38;5;66;03m# This is done with `_apply_feature_types`.\u001b[39;00m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mex_iterable:\n\u001b[1;32m--> 654\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m key, \u001b[43m_apply_feature_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\iterable_dataset.py:635\u001b[0m, in \u001b[0;36m_apply_feature_types\u001b[1;34m(example, features, token_per_repo_id)\u001b[0m\n\u001b[0;32m    633\u001b[0m encoded_example \u001b[38;5;241m=\u001b[39m features\u001b[38;5;241m.\u001b[39mencode_example(example)\n\u001b[0;32m    634\u001b[0m \u001b[38;5;66;03m# Decode example for Audio feature, e.g.\u001b[39;00m\n\u001b[1;32m--> 635\u001b[0m decoded_example \u001b[38;5;241m=\u001b[39m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoded_example\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded_example\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\features\\features.py:1794\u001b[0m, in \u001b[0;36mFeatures.decode_example\u001b[1;34m(self, example, token_per_repo_id)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;124;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \n\u001b[0;32m   1784\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;124;03m        :obj:`dict[str, Any]`\u001b[39;00m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m   1795\u001b[0m         column_name: decode_nested_example(feature, value, token_per_repo_id\u001b[38;5;241m=\u001b[39mtoken_per_repo_id)\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[0;32m   1797\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m column_name, (feature, value) \u001b[38;5;129;01min\u001b[39;00m zip_dict(\n\u001b[0;32m   1799\u001b[0m             {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example}, example\n\u001b[0;32m   1800\u001b[0m         )\n\u001b[0;32m   1801\u001b[0m     }\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\features\\features.py:1795\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;124;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \n\u001b[0;32m   1784\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;124;03m        :obj:`dict[str, Any]`\u001b[39;00m\n\u001b[0;32m   1792\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m-> 1795\u001b[0m         column_name: \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_column_requires_decoding[column_name]\n\u001b[0;32m   1797\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[0;32m   1798\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m column_name, (feature, value) \u001b[38;5;129;01min\u001b[39;00m zip_dict(\n\u001b[0;32m   1799\u001b[0m             {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example}, example\n\u001b[0;32m   1800\u001b[0m         )\n\u001b[0;32m   1801\u001b[0m     }\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\features\\features.py:1262\u001b[0m, in \u001b[0;36mdecode_nested_example\u001b[1;34m(schema, obj, token_per_repo_id)\u001b[0m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;66;03m# Object with special decoding:\u001b[39;00m\n\u001b[0;32m   1260\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image)):\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[1;32m-> 1262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\features\\audio.py:148\u001b[0m, in \u001b[0;36mAudio.decode_example\u001b[1;34m(self, value, token_per_repo_id)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn audio sample should have one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbytes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m but both are None in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmp3\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 148\u001b[0m     array, sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode_mp3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m path\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopus\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file:\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\features\\audio.py:331\u001b[0m, in \u001b[0;36mAudio._decode_mp3\u001b[1;34m(self, path_or_file)\u001b[0m\n\u001b[0;32m    329\u001b[0m     _librosa_warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     array, sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode_mp3_librosa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoding of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmp3\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m failed, probably because of streaming mode \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(`librosa` cannot decode \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmp3\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m file-like objects, only path-like).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    336\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\datasets\\features\\audio.py:365\u001b[0m, in \u001b[0;36mAudio._decode_mp3_librosa\u001b[1;34m(self, path_or_file)\u001b[0m\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    364\u001b[0m         _audioread_warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 365\u001b[0m     array, sampling_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmono\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmono\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m array, sampling_rate\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\librosa\\util\\decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     91\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     94\u001b[0m ]\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\librosa\\core\\audio.py:179\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    176\u001b[0m     y \u001b[38;5;241m=\u001b[39m to_mono(y)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr_native\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m     sr \u001b[38;5;241m=\u001b[39m sr_native\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\librosa\\util\\decorators.py:88\u001b[0m, in \u001b[0;36mdeprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     91\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(name, arg)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     94\u001b[0m ]\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\librosa\\core\\audio.py:647\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[0;32m    645\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m soxr\u001b[38;5;241m.\u001b[39mresample(y\u001b[38;5;241m.\u001b[39mT, orig_sr, target_sr, quality\u001b[38;5;241m=\u001b[39mres_type)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 647\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mresampy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_sr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fix:\n\u001b[0;32m    650\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mfix_length(y_hat, size\u001b[38;5;241m=\u001b[39mn_samples, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\whisper_event\\venv\\lib\\site-packages\\resampy\\core.py:117\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(x, sr_orig, sr_new, axis, filter, parallel, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m shape[axis] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(shape[axis] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mfloat\u001b[39m(sr_new) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(sr_orig))\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shape[axis] \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput signal length=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is too small to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresample from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m->\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(x\u001b[38;5;241m.\u001b[39mshape[axis], sr_orig, sr_new)\n\u001b[0;32m    120\u001b[0m     )\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Preserve contiguity of input (if it exists)\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(x\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger):\n",
      "\u001b[1;31mValueError\u001b[0m: Input signal length=0 is too small to resample from 32000->16000"
     ]
    }
   ],
   "source": [
    "dataset_loader = DatasetLoader([\"mozilla-foundation/common_voice_11_0\", \"google/fleurs\"], [\"sw\", \"sw_ke\"], [\"sentence\", \"transcription\"])\n",
    "dataset_loader.print_samples(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b049962-ca5f-4ae2-a544-ad69ea2ba703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 jua halina ganda gumu kama dunia ambayo unaweza kusimama kwayo. jua zima limetengenezwa kwa gesi moto na gesi ya utegili\n",
      "1 hata hivyo ugunduzi wa kaburi lake mnamo 1922 ulimfanya mtu mashuhuri. huku makaburi mengi ya zamani yakiporwa kaburi hili liliachwa karibu bila kusumbuliwa kamwe\n",
      "2 gurudumu limebadilisha dunia kwa njia za ajabu. jambo kubwa zaidi ambalo gurudumu limetufanyia ni kutupatia uchukuzi rahisi na wa haraka\n",
      "3 hakikisha kwamba basi unalofikiria kuabiri linaenda hebron na si katika makazi ya karibu ya kiyahudi ya kiryat arba tu\n",
      "4 wakati kupwa kwa maji kulifungua mwanya katika mto mystic katika kaskazini mashariki mwa rasi waliendeleza ua kwa haraka kwa ukuta mfupi wa mawe upande wa kaskazini na kuishia kwenye ukingo wa maji katika pwani ndogo\n",
      "5 mwanzoni aliipatia alfabeti ya hangeul jina la hunmin jeongeum kumaanisha  sauti sahihi za maagizo kwa watu\n",
      "6 uonaji au uwezo wa kuona hutegemea viungo vya hisia vya mfumo wa kuona au macho\n",
      "7 usilalie godoro au mto kwenye ardhi katika maeneo ambayo hujui wanyama wake wa kienyeji\n",
      "8 mbio kiasi za mashambani katika msimu wa baridi pamoja na shughuli katika ukumbi wa mazoezi kwa ajili ya sehemu ya juu ya mwili ndiyo maandalizi bora ya msimu wa mbio\n",
      "9 kwa kuzingatia umbali wa wapueblo wengi hutaweza kupata maisha ya usiku ya maana bila kusafiri kwenda albakuku au santa fe\n"
     ]
    }
   ],
   "source": [
    "dataset_names = [\"mozilla-foundation/common_voice_11_0\", \"google/fleurs\"]\n",
    "dataset_config_names = [\"sw\", \"sw_ke\"]\n",
    "text_column_names = [\"sentence\", \"transcription\"]\n",
    "\n",
    "splits = [\"train\" for i in range(len(dataset_names))]\n",
    "\n",
    "for i, dataset_name in tqdm(enumerate(dataset_names)):                         \n",
    "    dataset = load_dataset(dataset_names[i], dataset_config_names[i], split=splits[i], streaming=True)\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(CN.SAMPLE_RATE))\n",
    "    dataset = dataset.rename_column(text_column_names[i], \"sentence\")\n",
    "    dataset = dataset.remove_columns(set(dataset.features.keys()) - set([\"audio\", \"sentence\"]))\n",
    "\n",
    "for i, sample in enumerate(dataset):\n",
    "    print(i, sample[\"sentence\"])\n",
    "    if i == 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bed633c8-8c0c-48d3-9f0e-575191fd75ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multiple_streaming_datasets(\n",
    "    dataset_names: List,\n",
    "    dataset_config_names: List,\n",
    "    splits: Optional[List] = None,\n",
    "    text_column_names: Optional[List] = None,\n",
    "    sampling_rate: Optional[int] = 16000,\n",
    "    stopping_strategy: Optional[str] = \"all_exhausted\",\n",
    "    **kwargs\n",
    ") -> IterableDataset:\n",
    "\n",
    "    if len(dataset_names) != len(dataset_config_names):\n",
    "        raise ValueError(\n",
    "            f\"Ensure one config is passed for each dataset, got {len(dataset_names)} datasets and\"\n",
    "            f\" {len(dataset_config_names)} configs.\"\n",
    "        )\n",
    "\n",
    "    if splits is not None and len(splits) != len(dataset_names):\n",
    "        raise ValueError(\n",
    "            f\"Ensure one split is passed for each dataset, got {len(dataset_names)} datasets and {len(splits)} splits.\"\n",
    "        )\n",
    "\n",
    "    if text_column_names is not None and len(text_column_names) != len(dataset_names):\n",
    "        raise ValueError(\n",
    "            f\"Ensure one text column name is passed for each dataset, got {len(dataset_names)} datasets and\"\n",
    "            f\" {len(text_column_names)} text column names.\"\n",
    "        )\n",
    "\n",
    "    splits = splits if splits is not None else [\"train\" for i in range(len(dataset_names))]\n",
    "    text_column_names = (\n",
    "        text_column_names if text_column_names is not None else [\"text\" for i in range(len(dataset_names))]\n",
    "    )\n",
    "\n",
    "    all_datasets = []\n",
    "    # iterate over the datasets we want to interleave\n",
    "    for i, dataset_name in enumerate(dataset_names):\n",
    "        dataset = load_dataset(dataset_name, dataset_config_names[i], split=splits[i], streaming=True, **kwargs)\n",
    "        # resample to specified sampling rate\n",
    "        dataset = dataset.cast_column(\"audio\", Audio(sampling_rate))\n",
    "        #  normalise columns to [\"audio\", \"sentence\"]\n",
    "        if text_column_names[i] != \"sentence\":\n",
    "            dataset = dataset.rename_column(text_column_names[i], \"sentence\")\n",
    "        dataset = dataset.remove_columns(set(dataset.features.keys()) - set([\"audio\", \"sentence\"]))\n",
    "        all_datasets.append(dataset)\n",
    "\n",
    "    interleaved_dataset = interleave_datasets(all_datasets, stopping_strategy=stopping_strategy)\n",
    "    return interleaved_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99d8ea37-eabe-466e-8f3d-115b52050ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client_id': Value(dtype='string', id=None),\n",
       " 'path': Value(dtype='string', id=None),\n",
       " 'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),\n",
       " 'sentence': Value(dtype='string', id=None),\n",
       " 'up_votes': Value(dtype='int64', id=None),\n",
       " 'down_votes': Value(dtype='int64', id=None),\n",
       " 'age': Value(dtype='string', id=None),\n",
       " 'gender': Value(dtype='string', id=None),\n",
       " 'accent': Value(dtype='string', id=None),\n",
       " 'locale': Value(dtype='string', id=None),\n",
       " 'segment': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import IterableDatasetDict\n",
    "\n",
    "raw_datasets = IterableDatasetDict()\n",
    "raw_datasets[\"train\"] = load_streaming_dataset(\"mozilla-foundation/common_voice_11_0\", \"sw\", split=\"train\", use_auth_token=True)  # set split=\"train+validation\" for low-resource\n",
    "\n",
    "from transformers import WhisperProcessor\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Swahili\", task=\"transcribe\")\n",
    "\n",
    "raw_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6232d740-8932-418b-9e5c-a04e8bc03714",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bf62db0-66da-4f3f-8b85-b9522f82b031",
   "metadata": {},
   "outputs": [],
   "source": [
    "CN.LEARNING_RATE = 0.0005\n",
    "CN.WEIGHT_DECAY = 0.01\n",
    "CN.ADAM_EPSILON = 1e-8\n",
    "CN.WARMUP_STEPS = 2\n",
    "CN.BATCH_SIZE = 16\n",
    "CN.NUM_WORKER = 2\n",
    "CN.NUM_TRAIN_EPOCHS = 1\n",
    "CN.GRADIENT_ACCUMULATION_STEPS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8955d40c-fd6f-48f2-8234-bd62635f7e54",
   "metadata": {},
   "source": [
    "Let's create `WhisperModelModule` class that extends the `LightningModule` class from the PyTorch Lightning library. The class will initialize a whisper model for our chosen language and train only the decoder part of the model with a given dataset. The class will also define the forward method, training and validation steps, and the configuration of optimizers and schedulers. The class will log metrics such as loss, WER, and CER during training and validation.\n",
    "\n",
    "#### Under Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60252ee8-90ff-4675-b541-c43a295f35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperModelModule(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self):\n",
    "        return\n",
    "    \n",
    "    def training_step(self):\n",
    "        return\n",
    "    \n",
    "    def validation_step(self):\n",
    "        return\n",
    "    \n",
    "    def configure_optimizer(self):\n",
    "        return\n",
    "    \n",
    "    def setup(self):\n",
    "        return\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
